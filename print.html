<!DOCTYPE HTML>
<html lang="zh" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>OpenShift4 一步一脚印</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-6FWDB7021N"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-6FWDB7021N');
        </script>

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">介绍</a></li><li class="chapter-item expanded affix "><a href="thanks.html">献词</a></li><li class="chapter-item expanded "><a href="ocp4.9.install.html"><strong aria-hidden="true">1.</strong> openshift4.9 安装</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ocp4.9.install.1master.html"><strong aria-hidden="true">1.1.</strong> 单 master 集群安装</a></li><li class="chapter-item expanded "><a href="ocp4.9.install.sno.html"><strong aria-hidden="true">1.2.</strong> SNO 单节点集群安装</a></li><li class="chapter-item expanded "><a href="ocp4.9.active.rt.html"><strong aria-hidden="true">1.3.</strong> 激活实时操作系统</a></li></ol></li><li class="chapter-item expanded "><a href="nep.containerized.html"><strong aria-hidden="true">2.</strong> NEP应用容器化</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="nep.containerized.configmap.html"><strong aria-hidden="true">2.1.</strong> configmap方式注入license</a></li><li class="chapter-item expanded "><a href="nep.containerized.router.html"><strong aria-hidden="true">2.2.</strong> hostpath方式注入license</a></li><li class="chapter-item expanded "><a href="nep.containerized.systemd.html"><strong aria-hidden="true">2.3.</strong> 容器中使用systemd加载服务并注入env参数</a></li><li class="chapter-item expanded "><a href="nep.containerized.device.html"><strong aria-hidden="true">2.4.</strong> 设备驱动容器化方式加载</a></li><li class="chapter-item expanded "><a href="nep.containerized.image.html"><strong aria-hidden="true">2.5.</strong> 基础镜像制作</a></li><li class="chapter-item expanded "><a href="nep.containerized.helm.html"><strong aria-hidden="true">2.6.</strong> helm chart / helm operator 制作</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">OpenShift4 一步一脚印</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/wangzheng422/openshift4-steps-book" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="openshift4-一步一脚印"><a class="header" href="#openshift4-一步一脚印">Openshift4 一步一脚印</a></h1>
<p>作者由于工作关系，有很多的openshift4的经验积累，加上之前一直储备的 redhat openshift4 ，以及相关的平台和中间件的安装和使用的技巧和知识，想系统的整理一下，分享给读者。</p>
<h1 id="源代码和反馈"><a class="header" href="#源代码和反馈">源代码和反馈</a></h1>
<p>本书的源代码共享在github上，请在<a href="https://github.com/wangzheng422/openshift4-steps-book">这里</a>访问。如有问题，也请在<a href="https://github.com/wangzheng422/openshift4-steps-book/issues">github issues</a>上交流。</p>
<h1 id="许可证"><a class="header" href="#许可证">许可证</a></h1>
<p>本书和书中涉及代码采用GNU V3许可。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="献词"><a class="header" href="#献词">献词</a></h1>
<p>感谢老婆大人的全程支持。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift49-安装"><a class="header" href="#openshift49-安装">openshift4.9 安装</a></h1>
<p>这里讲述openshift从无到有的安装过程，目前的版本，聚焦在4.9. 红帽官方有详细的文档，还是要以红帽官方文档为准。不过官方文档比较庞杂，我们这里的文档，主要起到聚焦场景，简化官方文档不相干步骤，方便初学者上手和理解openshift。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-49-静态ip-半离线-单-master-baremetal-安装"><a class="header" href="#openshift-49-静态ip-半离线-单-master-baremetal-安装">openshift 4.9 静态IP 半离线 单 master baremetal 安装</a></h1>
<p>本文描述ocp4.9在baremetal(kvm模拟)上面，静态ip安装的方法。</p>
<p><img src="dia/4.9.bm.upi.1node.drawio.svg" alt="架构图" /></p>
<h2 id="离线安装包下载"><a class="header" href="#离线安装包下载">离线安装包下载</a></h2>
<p>打包好的安装包，在这里下载，百度盘下载链接，版本是 4.9.11 :</p>
<ul>
<li>链接: https://pan.baidu.com/s/1HxGdAp2ec4CZRVfeF8g35A 提取码: b4bl </li>
</ul>
<p>其中包括如下类型的文件：</p>
<ul>
<li>ocp4.tgz  这个文件包含了iso等安装介质，以及各种安装脚本，全部下载的镜像列表等。需要复制到宿主机，以及工具机上去。</li>
<li>registry.tgz  这个文件也是docker image registry的仓库打包文件。</li>
</ul>
<h2 id="宿主机准备"><a class="header" href="#宿主机准备">宿主机准备</a></h2>
<p>本次实验，是在一个32C， 256G 的主机上面，用很多个虚拟机安装测试。所以先准备这个宿主机。</p>
<p>如果是多台宿主机，记得一定要调整时间配置，让这些宿主机的时间基本一致，否则证书会出问题。</p>
<p>主要的准备工作有</p>
<ul>
<li>配置yum源</li>
<li>配置dns</li>
<li>安装镜像仓库</li>
<li>配置vnc环境</li>
<li>配置kvm需要的网络</li>
<li>创建helper kvm</li>
<li>配置一个haproxy，从外部导入流量给kvm</li>
</ul>
<p>以上准备工作，dns部分需要根据实际项目环境有所调整。</p>
<p>本次的宿主机是两台rocky linux</p>
<h2 id="kvm-host-101"><a class="header" href="#kvm-host-101">kvm host 101</a></h2>
<pre><code class="language-bash">
# 因为是半离线，我们的host os还有helper os是联线的，那么我们就用在线的源吧。
dnf install -y epel-release

dnf install -y byobu htop dstat

# 准备vnc环境
vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/config
session=gnome
securitytypes=vncauth,tlsvnc
# desktop=sandbox
geometry=1280x800
alwaysshared
EOF

cat &lt;&lt; EOF &gt;&gt; /etc/tigervnc/vncserver.users
:1=root
EOF

# systemctl disable vncserver@:1
systemctl start vncserver@:1
# 如果你想停掉vnc server，这么做
systemctl stop vncserver@:1

# /usr/libexec/vncsession-start :1

# 配置kvm环境
dnf -y groupinstall &quot;Server with GUI&quot;

dnf -y install qemu-kvm libvirt libguestfs-tools virt-install virt-viewer virt-manager tigervnc-server

systemctl disable --now firewalld
systemctl enable --now libvirtd

# 创建实验用虚拟网络

mkdir -p /data/kvm
cat &lt;&lt; 'EOF' &gt; /data/kvm/bridge.sh
#!/usr/bin/env bash

PUB_CONN='eno1'
PUB_IP='172.21.6.104/24'
PUB_GW='172.21.6.254'
PUB_DNS='172.21.1.1'

nmcli con down &quot;$PUB_CONN&quot;
nmcli con delete &quot;$PUB_CONN&quot;
nmcli con down baremetal
nmcli con delete baremetal
# RHEL 8.1 appends the word &quot;System&quot; in front of the connection,delete in case it exists
nmcli con down &quot;System $PUB_CONN&quot;
nmcli con delete &quot;System $PUB_CONN&quot;
nmcli connection add ifname baremetal type bridge con-name baremetal ipv4.method 'manual' \
    ipv4.address &quot;$PUB_IP&quot; \
    ipv4.gateway &quot;$PUB_GW&quot; \
    ipv4.dns &quot;$PUB_DNS&quot;
    
nmcli con add type bridge-slave ifname &quot;$PUB_CONN&quot; master baremetal
nmcli con down &quot;$PUB_CONN&quot;;pkill dhclient;dhclient baremetal
nmcli con up baremetal
EOF
bash /data/kvm/bridge.sh

# 创建工具机

mkdir -p /data/kvm
cd /data/kvm

osinfo-query os | grep rhel8
#  rhel8-unknown        | Red Hat Enterprise Linux 8 Unknown                 | 8-unknown | http://redhat.com/rhel/8-unknown
#  rhel8.0              | Red Hat Enterprise Linux 8.0                       | 8.0      | http://redhat.com/rhel/8.0
#  rhel8.1              | Red Hat Enterprise Linux 8.1                       | 8.1      | http://redhat.com/rhel/8.1
#  rhel8.2              | Red Hat Enterprise Linux 8.2                       | 8.2      | http://redhat.com/rhel/8.2
#  rhel8.3              | Red Hat Enterprise Linux 8.3                       | 8.3      | http://redhat.com/rhel/8.3
#  rhel8.4              | Red Hat Enterprise Linux 8.4                       | 8.4      | http://redhat.com/rhel/8.4

wget https://mirrors.sjtug.sjtu.edu.cn/rocky/8.4/isos/x86_64/Rocky-8.4-x86_64-minimal.iso

# lvremove -f rhel/data
lvcreate -y -l 100%FREE -n data nvme
mkfs.xfs /dev/nvme/data
mkdir -p /data/nvme
mount /dev/nvme/data /data/nvme

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/nvme/data /data/nvme                   xfs     defaults        0 0
EOF

cd /data/kvm
wget https://mirrors.sjtug.sjtu.edu.cn/rocky/8.4/isos/x86_64/Rocky-8.4-x86_64-minimal.iso

export http_proxy=&quot;http://192.168.195.54:5085&quot;
export https_proxy=${http_proxy}

wget https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.8/scripts/helper-ks-rocky.cfg

unset http_proxy
unset https_proxy

sed -i '0,/^network.*/s/^network.*/network  --bootproto=static --device=enp1s0 --gateway=172.21.6.254 --ip=172.21.6.11  --netmask=255.255.255.0 --nameserver=172.21.1.1  --ipv6=auto --activate/' helper-ks-rocky.cfg

virt-install --name=&quot;ocp4-aHelper&quot; --vcpus=2 --ram=8192 \
--cpu=host-model \
--disk path=/data/kvm/ocp4-aHelper.qcow2,bus=virtio,size=150 \
--os-variant rhel8.4 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59000 \
--boot menu=on --location /data/kvm/Rocky-8.4-x86_64-minimal.iso \
--initrd-inject helper-ks-rocky.cfg --extra-args &quot;inst.ks=file:/helper-ks-rocky.cfg&quot; 

# virt-viewer --domain-name ocp4-aHelper
# virsh start ocp4-aHelper
# virsh list --all

# start chrony/ntp server on host
# cat &lt;&lt; EOF &gt; /etc/chrony.conf
# driftfile /var/lib/chrony/drift
# makestep 1.0 3
# rtcsync
# allow 192.0.0.0/8
# local stratum 10
# logdir /var/log/chrony
# EOF

# echo &quot;allow 192.0.0.0/8&quot; &gt;&gt; /etc/chrony.conf
# systemctl enable --now chronyd
# # systemctl restart chronyd
# chronyc tracking
# chronyc sources -v
# chronyc sourcestats -v
# chronyc makestep

</code></pre>
<h2 id="工具机准备"><a class="header" href="#工具机准备">工具机准备</a></h2>
<p>以下是在工具机里面，进行的安装操作。</p>
<p>主要的操作有</p>
<ul>
<li>配置yum源</li>
<li>运行ansible脚本，自动配置工具机</li>
<li>上传定制的安装配置文件</li>
<li>生成ignition文件</li>
</ul>
<pre><code class="language-bash">
dnf install -y epel-release
dnf install -y byobu
dnf update -y
reboot

sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config
systemctl restart sshd

cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

echo &quot;allow 192.0.0.0/8&quot; &gt;&gt; /etc/chrony.conf
systemctl enable --now chronyd
# systemctl restart chronyd
chronyc tracking
chronyc sources -v
chronyc sourcestats -v
chronyc makestep

nmcli con mod enp1s0 +ipv4.addresses &quot;192.168.7.11/24&quot;
nmcli con up enp1s0

dnf -y install ansible git unzip podman python3 buildah skopeo

mkdir -p /data/ocp4/
# scp ocp4.tgz to /data
# scp * root@172.21.6.11:/data/
cd /data
tar zvxf ocp.*.tgz
tar zvxf registry.*.tgz
cd /data/ocp4

rm -f /data/*.tgz

# 配置registry
# 配置registry
mkdir -p /etc/crts/ &amp;&amp; cd /etc/crts

# https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.1/html/installation_guide/installing-codeready-workspaces-in-tls-mode-with-self-signed-certificates_crw
openssl genrsa -out /etc/crts/redhat.ren.ca.key 4096
openssl req -x509 \
  -new -nodes \
  -key /etc/crts/redhat.ren.ca.key \
  -sha256 \
  -days 36500 \
  -out /etc/crts/redhat.ren.ca.crt \
  -subj /CN=&quot;Local Red Hat Ren Signer&quot; \
  -reqexts SAN \
  -extensions SAN \
  -config &lt;(cat /etc/pki/tls/openssl.cnf \
      &lt;(printf '[SAN]\nbasicConstraints=critical, CA:TRUE\nkeyUsage=keyCertSign, cRLSign, digitalSignature'))

openssl genrsa -out /etc/crts/redhat.ren.key 2048

openssl req -new -sha256 \
    -key /etc/crts/redhat.ren.key \
    -subj &quot;/O=Local Red Hat Ren /CN=*.ocp4.redhat.ren&quot; \
    -reqexts SAN \
    -config &lt;(cat /etc/pki/tls/openssl.cnf \
        &lt;(printf &quot;\n[SAN]\nsubjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;)) \
    -out /etc/crts/redhat.ren.csr

openssl x509 \
    -req \
    -sha256 \
    -extfile &lt;(printf &quot;subjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;) \
    -days 365 \
    -in /etc/crts/redhat.ren.csr \
    -CA /etc/crts/redhat.ren.ca.crt \
    -CAkey /etc/crts/redhat.ren.ca.key \
    -CAcreateserial -out /etc/crts/redhat.ren.crt

openssl x509 -in /etc/crts/redhat.ren.crt -text

/bin/cp -f /etc/crts/redhat.ren.ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

cd /data
# mkdir -p /data/registry
# tar zxf registry.tgz
dnf -y install podman pigz skopeo jq 
# pigz -dc registry.tgz | tar xf -
cd /data/ocp4
podman load -i /data/ocp4/registry.tgz

systemctl disable --now firewalld

podman run --restart always --name local-registry -p 5443:5443 \
  -d --restart=always \
  -v /home/ocp.4.9.5/registry/:/var/lib/registry:z \
  -v /etc/crts:/certs:z \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:5443 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/redhat.ren.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/redhat.ren.key \
  docker.io/library/registry:2

podman start local-registry

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/building_running_and_managing_containers/assembly_porting-containers-to-systemd-using-podman_building-running-and-managing-containers

# podman generate systemd --new --files --name local-registry
podman generate systemd --files --name local-registry
# /root/container-local-registry.service
cp -Z container-local-registry.service  /usr/lib/systemd/system

systemctl enable --now container-local-registry.service
systemctl status container-local-registry.service

# podman rm --storage 7cb9fcea76ad384313a682a469be6784786eb5004a190ad2abe68978b1566416

# firewall-cmd --permanent --add-port=5443/tcp
# firewall-cmd --reload

# 加载更多的镜像
# 解压缩 ocp4.tgz
# bash add.image.load.sh /data/4.6.5/install.image 'registry.ocp4.redhat.ren:5443'

# https://github.com/christianh814/ocp4-upi-helpernode/blob/master/docs/quickstart.md

# in helper node
# mkdir /etc/yum.repos.d.bak
# mv /etc/yum.repos.d/* /etc/yum.repos.d.bak/
# cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
# [remote]
# name=RHEL FTP
# baseurl=ftp://192.168.7.1/data
# enabled=1
# gpgcheck=0

# EOF

# yum clean all
# yum repolist

# 这里使用了一个ansible的项目，用来部署helper节点的服务。
# https://github.com/wangzheng422/ocp4-upi-helpernode
unzip ocp4-upi-helpernode.zip
# 这里使用了一个ignition文件合并的项目，用来帮助自定义ignition文件。
# https://github.com/wangzheng422/filetranspiler
# podman load -i filetranspiler.tgz

# podman load -i nexus.tgz

ssh-keygen

# 接下来，我们使用ansible来配置helper节点，装上各种openshift集群需要的服务
# 根据现场环境，修改 ocp4-upi-helpernode-master/vars-static.yaml
# 主要是修改各个节点的网卡和硬盘参数，还有IP地址
cd /data/ocp4/ocp4-upi-helpernode-master
# ansible-playbook -e @vars-static.yaml -e '{staticips: true}' tasks/main.yml

cat &lt;&lt; 'EOF' &gt; /data/ocp4/ocp4-upi-helpernode-master/vars.yaml
---
ocp_version: 4.9.5
ssh_gen_key: false
staticips: true
firewalld: false
dns_forward: yes
iso:
  iso_dl_url: &quot;file:///data/ocp4/rhcos-live.x86_64.iso&quot;
  my_iso: &quot;rhcos-live.iso&quot; # this is internal file, just leave as it.
helper:
  name: &quot;helper&quot;
  ipaddr: &quot;192.168.7.11&quot;
  networkifacename: &quot;enp1s0&quot;
  gateway: &quot;192.168.7.1&quot;
  netmask: &quot;255.255.255.0&quot;
dns:
  domain: &quot;redhat.ren&quot;
  clusterid: &quot;ocp4&quot;
  forwarder1: &quot;202.106.0.20&quot;
  forwarder2: &quot;202.106.0.20&quot;
bootstrap:
  name: &quot;bootstrap&quot;
  ipaddr: &quot;192.168.7.12&quot;
  interface: &quot;enp1s0&quot;
  install_drive: &quot;vda&quot;
masters:
  - name: &quot;master-0&quot;
    ipaddr: &quot;192.168.7.13&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;
  - name: &quot;master-1&quot;
    ipaddr: &quot;192.168.7.14&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;    
  - name: &quot;master-2&quot;
    ipaddr: &quot;192.168.7.15&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;    
workers:
  - name: &quot;worker-0&quot;
    ipaddr: &quot;192.168.7.16&quot;
    interface: &quot;ens18f0&quot;
    install_drive: &quot;sda&quot;
  - name: &quot;worker-1&quot;
    ipaddr: &quot;192.168.7.17&quot;
    interface: &quot;eno2&quot;
    install_drive: &quot;sda&quot;
  # - name: &quot;worker-2&quot;
  #   ipaddr: &quot;192.168.7.18&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
  # - name: &quot;infra-0&quot;
  #   ipaddr: &quot;192.168.7.19&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
  # - name: &quot;infra-1&quot;
  #   ipaddr: &quot;192.168.7.20&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
  # - name: &quot;worker-3&quot;
  #   ipaddr: &quot;192.168.7.21&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
  # - name: &quot;worker-4&quot;
  #   ipaddr: &quot;192.168.7.22&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
others:
  - name: &quot;registry&quot;
    ipaddr: &quot;192.168.7.1&quot;
  - name: &quot;yum&quot;
    ipaddr: &quot;192.168.7.1&quot;
  - name: &quot;quay&quot;
    ipaddr: &quot;192.168.7.11&quot;
  - name: &quot;nexus&quot;
    ipaddr: &quot;192.168.7.1&quot;
  - name: &quot;git&quot;
    ipaddr: &quot;192.168.7.11&quot;
otherdomains:
  - domain: &quot;rhv.redhat.ren&quot;
    hosts:
    - name: &quot;manager&quot;
      ipaddr: &quot;192.168.7.71&quot;
    - name: &quot;rhv01&quot;
      ipaddr: &quot;192.168.7.72&quot;
  - domain: &quot;sno.redhat.ren&quot;
    hosts:
    - name: &quot;*&quot;
      ipaddr: &quot;192.168.7.71&quot;
    - name: &quot;*.apps&quot;
      ipaddr: &quot;192.168.7.71&quot;
force_ocp_download: false
remove_old_config_files: false
ocp_client: &quot;file:///data/ocp4/{{ ocp_version }}/openshift-client-linux-{{ ocp_version }}.tar.gz&quot;
ocp_installer: &quot;file:///data/ocp4/{{ ocp_version }}/openshift-install-linux-{{ ocp_version }}.tar.gz&quot;
ppc64le: false
arch: 'x86_64'
chronyconfig:
  enabled: true
  content:
    - server: &quot;192.168.7.1&quot;
      options: iburst
setup_registry: # don't worry about this, just leave it here
  deploy: false
  registry_image: docker.io/library/registry:2
  local_repo: &quot;ocp4/openshift4&quot;
  product_repo: &quot;openshift-release-dev&quot;
  release_name: &quot;ocp-release&quot;
  release_tag: &quot;4.6.1-x86_64&quot;
ocp_filetranspiler: &quot;file:///data/ocp4/filetranspiler.tgz&quot;
registry_server: &quot;registry.ocp4.redhat.ren:5443&quot;
EOF

ansible-playbook -e @vars.yaml tasks/main.yml

# try this:
/usr/local/bin/helpernodecheck

mkdir -p /data/install

# # GOTO image registry host
# # copy crt files to helper node
# scp /etc/crts/redhat.ren.ca.crt root@192.168.7.11:/data/install/
# scp /etc/crts/redhat.ren.crt root@192.168.7.11:/data/install/
# scp /etc/crts/redhat.ren.key root@192.168.7.11:/data/install/

# GO back to help node
# /bin/cp -f /data/install/redhat.ren.crt /etc/pki/ca-trust/source/anchors/
# update-ca-trust extract

# 定制ignition
cd /data/install

# 根据现场环境，修改 install-config.yaml
# 至少要修改ssh key， 还有 additionalTrustBundle，这个是镜像仓库的csr 

# vi install-config.yaml 
cat &lt;&lt; EOF &gt; /data/install/install-config.yaml 
apiVersion: v1
baseDomain: redhat.ren
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 0
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 1
metadata:
  name: ocp4
networking:
  clusterNetworks:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
pullSecret: '{&quot;auths&quot;:{&quot;registry.ocp4.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;},&quot;registry.ppa.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;}}}'
sshKey: |
$( cat /root/.ssh/id_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /etc/crts/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  - registry.ocp4.redhat.ren:5443/ocp4/release
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  - registry.ocp4.redhat.ren:5443/ocp4/release
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

cd /data/install/
/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] 

openshift-install create manifests --dir=/data/install

# copy ntp related config
/bin/cp -f  /data/ocp4/ocp4-upi-helpernode-master/machineconfig/* /data/install/openshift/

# copy image registry proxy related config
cd /data/ocp4
bash image.registries.conf.sh nexus.ocp4.redhat.ren:8083

/bin/cp -f /data/ocp4/image.registries.conf /etc/containers/registries.conf.d/

/bin/cp -f /data/ocp4/99-worker-container-registries.yaml /data/install/openshift
/bin/cp -f /data/ocp4/99-master-container-registries.yaml /data/install/openshift

cd /data/install/
openshift-install create ignition-configs --dir=/data/install

cd /data/ocp4/ocp4-upi-helpernode-master
# 我们来为每个主机，复制自己版本的ign，并复制到web server的目录下
ansible-playbook -e @vars.yaml tasks/ign.yml
# 如果对每个主机有自己ign的独特需求，在这一步，去修改ign。

# 以下操作本来是想设置网卡地址，但是实践发现是不需要的。
# 保留在这里，是因为他可以在安装的时候注入文件，非常有用。
# mkdir -p bootstrap/etc/sysconfig/network-scripts/
# cat &lt;&lt;EOF &gt; bootstrap/etc/sysconfig/network-scripts/ifcfg-ens3
# DEVICE=ens3
# BOOTPROTO=none
# ONBOOT=yes
# IPADDR=192.168.7.12
# NETMASK=255.255.255.0
# GATEWAY=192.168.7.1
# DNS=192.168.7.11
# DNS1=192.168.7.11
# DNS2=192.168.7.1
# DOMAIN=redhat.ren
# PREFIX=24
# DEFROUTE=yes
# IPV6INIT=no
# EOF
# filetranspiler -i bootstrap.ign -f bootstrap -o bootstrap-static.ign
# /bin/cp -f bootstrap-static.ign /var/www/html/ignition/

# 我们为每个节点创建各自的iso文件
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars.yaml tasks/iso.yml

</code></pre>
<h2 id="回到宿主机"><a class="header" href="#回到宿主机">回到宿主机</a></h2>
<p>本来，到了这一步，就可以开始安装了，但是我们知道coreos装的时候，要手动输入很长的命令行，实际操作的时候，那是不可能输入对的，输入错一个字符，安装就失败，要重启，重新输入。。。</p>
<p>为了避免这种繁琐的操作，参考网上的做法，我们就需要为每个主机定制iso了。好在，之前的步骤，我们已经用ansible创建了需要的iso，我们把这些iso复制到宿主机上，就可以继续了。</p>
<p>这里面有一个坑，我们是不知道主机的网卡名称的，只能先用coreos iso安装启动一次，进入单用户模式以后，ip a 来查看以下，才能知道，一般来说，是ens3。</p>
<p>另外，如果是安装物理机，disk是哪个，也需要上述的方法，来看看具体的盘符。另外，推荐在物理机上安装rhel 8 来测试一下物理机是不是支持coreos。物理机安装的时候，遇到不写盘的问题，可以尝试添加启动参数： ignition.firstboot=1</p>
<pre><code class="language-bash"># on kvm host 172.21.6.101
export KVM_DIRECTORY=/home/data/kvm

mkdir -p  ${KVM_DIRECTORY}
cd ${KVM_DIRECTORY}
scp root@192.168.7.11:/data/install/{*boot*,*master-0,*worker-0}.iso ${KVM_DIRECTORY}/

# on kvm host 172.21.6.101

# finally, we can start install :)
# 你可以一口气把虚拟机都创建了，然后喝咖啡等着。
# 从这一步开始，到安装完毕，大概30分钟。
# export KVM_DIRECTORY=/data/kvm
virt-install --name=ocp4-bootstrap --vcpus=4 --ram=8192 \
--disk path=${KVM_DIRECTORY}/ocp4-bootstrap.qcow2,bus=virtio,size=50 \
--os-variant rhel8.3 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59001 \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-bootstrap.iso   

# 想登录进coreos一探究竟？那么这么做
# ssh core@bootstrap 
# journalctl -b -f -u bootkube.service
# export KVM_DIRECTORY=/data/kvm
virt-install --name=ocp4-master-0 --vcpus=16 --ram=49152 \
--cpu=host-model \
--disk path=${KVM_DIRECTORY}/ocp4-master-0.qcow2,bus=virtio,size=120 \
--os-variant rhel8.3 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59002 \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-0.iso 

# virt-install --name=ocp4-master-1 --vcpus=10 --ram=20480 \
# --cpu=host-model \
# --disk path=/data/nvme/ocp4-master-1.qcow2,bus=virtio,size=120 \
# --os-variant rhel8.4 --network bridge=baremetal,model=virtio \
# --graphics vnc,port=59003 \
# --boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-1.iso 

# # ssh core@192.168.7.13

# # on kvm host 172.21.6.103
# export KVM_DIRECTORY=/data/kvm

# virt-install --name=ocp4-master-2 --vcpus=22 --ram=30720 \
# --cpu=host-model \
# --disk path=/data/kvm/ocp4-master-2.qcow2,bus=virtio,size=120 \
# --os-variant rhel8.4 --network bridge=baremetal,model=virtio \
# --graphics vnc,port=59004 \
# --boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-2.iso 

# on kvm host 172.21.6.104
export KVM_DIRECTORY=/data/kvm

# virt-install --name=ocp4-worker-0 --vcpus=4 --ram=10240 \
# --disk path=/data/kvm/ocp4-worker-0.qcow2,bus=virtio,size=120 \
# --os-variant rhel8.4 --network bridge=baremetal,model=virtio \
# --graphics vnc,port=59005 \
# --boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-worker-0.iso 


# on workstation
# open http://192.168.7.11:9000/
# to check

# if you want to stop or delete vm, try this
virsh list --all
virsh destroy ocp4-bootstrap
virsh destroy ocp4-master-0 
# virsh destroy ocp4-master-1 
# virsh destroy ocp4-master-2 
# virsh destroy ocp4-worker0 
# virsh destroy ocp4-worker1 
# virsh destroy ocp4-worker2
virsh undefine ocp4-bootstrap --remove-all-storage
virsh undefine ocp4-master-0 --remove-all-storage
# virsh undefine ocp4-master-1 --remove-all-storage
# virsh undefine ocp4-master-2 --remove-all-storage
# virsh undefine ocp4-worker0 
# virsh undefine ocp4-worker1 
# virsh undefine ocp4-worker2

</code></pre>
<h2 id="在工具机上面"><a class="header" href="#在工具机上面">在工具机上面</a></h2>
<p>这个时候，安装已经自动开始了，我们只需要回到工具机上静静的观察就可以了。</p>
<p>在bootstrap和装master阶段，用这个命令看进度。</p>
<pre><code class="language-bash">cd /data/install
export KUBECONFIG=/data/install/auth/kubeconfig
echo &quot;export KUBECONFIG=/data/install/auth/kubeconfig&quot; &gt;&gt; ~/.bashrc
oc completion bash | sudo tee /etc/bash_completion.d/openshift &gt; /dev/null

cd /data/install
openshift-install wait-for bootstrap-complete --log-level debug

</code></pre>
<p>有时候证书会过期，验证方法是登录 bootstrap, 看看过期时间。如果确定过期，要清除所有的openshift-install生成配置文件的缓存，重新来过。</p>
<pre><code class="language-bash">echo | openssl s_client -connect localhost:6443 | openssl x509 -noout -text | grep Not
</code></pre>
<p>一般来说，如果在openshift-install这一步之前，按照文档，删除了缓存文件，就不会出现过期的现象。</p>
<pre><code class="language-bash">oc get nodes
</code></pre>
<p>这个时候，只能看到master，是因为worker的csr没有批准。如果虚拟机是一口气创建的，那么多半不会遇到下面的问题。</p>
<pre><code class="language-bash">oc get csr
</code></pre>
<p>会发现有很多没有被批准的</p>
<p>批准之</p>
<pre><code class="language-bash">yum -y install jq
oc get csr | grep -v Approved
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve
# oc get csr -o name | xargs oc adm certificate approve
</code></pre>
<p>然后worker 节点cpu飙高，之后就能看到worker了。</p>
<p>上面的操作完成以后，就可以完成最后的安装了</p>
<pre><code class="language-bash">openshift-install wait-for install-complete --log-level debug
# here is the output
# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.redhat.ren
# INFO Login to the console with user: &quot;kubeadmin&quot;, and password: &quot;fIiak-IaQLh-fUYHV-fjpQW&quot;

# we are testing env, so we don't need ingress replicas.
oc patch --namespace=openshift-ingress-operator --patch='{&quot;spec&quot;: {&quot;replicas&quot;: 1}}' --type=merge ingresscontroller/default

</code></pre>
<h2 id="镜像仓库代理--image-registry-proxy"><a class="header" href="#镜像仓库代理--image-registry-proxy">镜像仓库代理 / image registry proxy</a></h2>
<p>准备离线镜像仓库非常麻烦，好在我们找到了一台在线的主机，那么我们可以使用nexus构造image registry proxy，在在线环境上面，做一遍PoC，然后就能通过image registry proxy得到离线镜像了</p>
<ul>
<li>https://mtijhof.wordpress.com/2018/07/23/using-nexus-oss-as-a-proxy-cache-for-docker-images/</li>
</ul>
<h2 id="配置镜像仓库的ca"><a class="header" href="#配置镜像仓库的ca">配置镜像仓库的ca</a></h2>
<p>安装过程里面，已经把镜像仓库的ca放进去了，但是好想image stream不认，让我们再试试</p>
<pre><code class="language-bash">oc project openshift-config
oc create configmap ca.for.registry -n openshift-config \
    --from-file=registry.ocp4.redhat.ren..5443=/etc/crts/redhat.ren.ca.crt 
#     --from-file=nexus.ocp4.redhat.ren..8083=/data/install/redhat.ren.ca.crt 

oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;additionalTrustedCA&quot;:{&quot;name&quot;:&quot;ca.for.registry&quot;}}}'  --type=merge

oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;registrySources&quot;:{&quot;insecureRegistries&quot;:[&quot;nexus.ocp4.redhat.ren:8083&quot;]}}}'  --type=merge

oc get image.config.openshift.io/cluster -o yaml

# openshift project下面的image stream重新加载一下把
oc get is -o json | jq -r '.items[].metadata.name' | xargs -L1 oc import-image --all 

</code></pre>
<h2 id="配置internal-registry"><a class="header" href="#配置internal-registry">配置internal registry</a></h2>
<p>我们的工具机是带nfs的，那么就给interneal registry配置高档一些的nfs存储吧，不要用emptydir</p>
<pre><code class="language-bash">bash /data/ocp4/ocp4-upi-helpernode-master/files/nfs-provisioner-setup.sh

# oc edit configs.imageregistry.operator.openshift.io
# 修改 storage 部分
# storage:
#   pvc:
#     claim:
oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;,&quot;storage&quot;:{&quot;pvc&quot;:{&quot;claim&quot;:&quot;&quot;}}}}' --type=merge

# if you want to restore
oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

oc get clusteroperator image-registry

oc get configs.imageregistry.operator.openshift.io cluster -o yaml

# 把imagepruner给停掉
# https://bugzilla.redhat.com/show_bug.cgi?id=1852501#c24
# oc patch imagepruner.imageregistry/cluster --patch '{&quot;spec&quot;:{&quot;suspend&quot;:true}}' --type=merge
# oc -n openshift-image-registry delete jobs --all
</code></pre>
<h2 id="配置sample-operator"><a class="header" href="#配置sample-operator">配置sample operator</a></h2>
<p>openshift内置了一个sample operator，里面有一大堆红帽的产品。</p>
<pre><code class="language-bash">oc get configs.samples.operator.openshift.io/cluster -o yaml

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;, &quot;samplesRegistry&quot;: &quot;nexus.ocp4.redhat.ren:8083&quot;}}' --type=merge

# if you want to restore
oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Unmanaged&quot;}}' --type=merge

# if you want to get ride of sampe operator
oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

</code></pre>
<h2 id="chronyntp-设置"><a class="header" href="#chronyntp-设置">chrony/NTP 设置</a></h2>
<p>在 ocp 4.6 里面，需要设定ntp同步，我们之前ansible脚本，已经创建好了ntp的mco配置，把他打到系统里面就好了。</p>
<pre><code class="language-bash">oc apply -f /data/ocp4/ocp4-upi-helpernode-master/machineconfig/

</code></pre>
<h2 id="operator-hub-离线安装"><a class="header" href="#operator-hub-离线安装">Operator Hub 离线安装</a></h2>
<p>使用nexus作为image proxy以后，就不需要做这个离线操作了。有些情况下，我们可能还需要屏蔽掉默认的operator hub</p>
<pre><code class="language-bash">
oc patch OperatorHub cluster --type json \
    -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true}]'

oc get OperatorHub cluster -o yaml

oc get OperatorHub
# NAME      AGE
# cluster   20h
</code></pre>
<h2 id="给-openshift-project-image-stream-打补丁"><a class="header" href="#给-openshift-project-image-stream-打补丁">给 openshift project image stream 打补丁</a></h2>
<p>在有代理的网络环境中，我们需要给openshift project下的image stream打一些补丁。</p>
<pre><code class="language-bash">cd /data/ocp4
bash is.patch.sh registry.ocp4.redhat.ren:5443/ocp4/openshift4

</code></pre>
<h2 id="disable-offical-helm-chart--enable-helm-proxy"><a class="header" href="#disable-offical-helm-chart--enable-helm-proxy">disable offical helm chart &amp; enable helm proxy</a></h2>
<p>我们是半离线环境，所以openshift4内置的官方helm chart是无法访问的，我们禁用之。</p>
<pre><code class="language-bash">oc get HelmChartRepository
# NAME               AGE
# redhat-helm-repo   19h

# oc patch HelmChartRepository redhat-helm-repo --type json \
#     -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disabled&quot;, &quot;value&quot;: true}]'

oc patch  --patch='{&quot;spec&quot;: {&quot;disabled&quot;: true}}' --type=merge HelmChartRepository/openshift-helm-charts

cat &lt;&lt; EOF &gt; /data/install/helm.ocp.yaml
apiVersion: helm.openshift.io/v1beta1
kind: HelmChartRepository
metadata:
  name: openshift-helm-charts-wzh
spec:
 # optional name that might be used by console
  name: openshift-helm-charts-wzh
  connectionConfig:
    url: http://nexus.ocp4.redhat.ren:8082/repository/charts.openshift.io/
EOF
oc create -f /data/install/helm.ocp.yaml

</code></pre>
<h2 id="给-router--ingress-更换证书"><a class="header" href="#给-router--ingress-更换证书">给 router / ingress 更换证书</a></h2>
<p>有时候，我们需要公网CA认证的证书，给router来用，那么我们就搞一下</p>
<p>https://docs.openshift.com/container-platform/4.6/security/certificates/replacing-default-ingress-certificate.html</p>
<pre><code class="language-bash">
mkdir -p /data/ccn/ingress-keys/etc
mkdir -p /data/ccn/ingress-keys/lib
cd /data/ccn/ingress-keys
podman run -it --rm --name certbot \
            -v &quot;/data/ccn/ingress-keys/etc:/etc/letsencrypt&quot;:Z \
            -v &quot;/data/ccn/ingress-keys/lib:/var/lib/letsencrypt&quot;:Z \
            docker.io/certbot/certbot certonly  -d &quot;*.apps.ocp4.redhat.ren&quot; --manual --preferred-challenges dns-01  --server https://acme-v02.api.letsencrypt.org/directory

cp ./etc/archive/apps.ocp4.redhat.ren/fullchain1.pem apps.ocp4.redhat.ren.crt
cp ./etc/archive/apps.ocp4.redhat.ren/privkey1.pem apps.ocp4.redhat.ren.key

ssh root@192.168.7.11 mkdir -p /data/install/ingress-key

scp apps.* root@192.168.7.11:/data/install/ingress-key

# on helper
cd /data/install/ingress-key

oc create secret tls wzh-ingress-key \
     --cert=apps.ocp4.redhat.ren.crt \
     --key=apps.ocp4.redhat.ren.key \
     -n openshift-ingress

oc patch ingresscontroller.operator default \
     --type=merge -p \
     '{&quot;spec&quot;:{&quot;defaultCertificate&quot;: {&quot;name&quot;: &quot;wzh-ingress-key&quot;}}}' \
     -n openshift-ingress-operator

</code></pre>
<h2 id="更改系统默认时区"><a class="header" href="#更改系统默认时区">更改系统默认时区</a></h2>
<p>https://access.redhat.com/solutions/5487331</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /data/install/timezone.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: worker-custom-timezone-configuration
spec:
  config:
    ignition:
      version: 2.2.0
    systemd:
      units:
      - contents: |
          [Unit]
          Description=set timezone
          After=network-online.target

          [Service]
          Type=oneshot
          ExecStart=timedatectl set-timezone Asia/Shanghai

          [Install]
          WantedBy=multi-user.target
        enabled: true
        name: custom-timezone.service
EOF
oc create -f /data/install/timezone.yaml

</code></pre>
<h2 id="排错技巧"><a class="header" href="#排错技巧">排错技巧</a></h2>
<pre><code class="language-bash">
# login to bootstrap to debug
# find the ip from kvm console
ssh -i ~/.ssh/helper_rsa core@192.168.7.75
journalctl -b -f -u release-image.service -u bootkube.service
journalctl -b -u release-image.service -u bootkube.service | grep -i baremetal
sudo -i
export KUBECONFIG=/etc/kubernetes/kubeconfig
oc get pod -n openshift-machine-api
oc get BareMetalHost -n openshift-machine-api

# debug why bootstrap can't be ping...
cat .openshift_install_state.json | jq  '.&quot;*bootstrap.Bootstrap&quot;'.Config.storage.files[].path

cat .openshift_install_state.json | jq -r '.&quot;*bootstrap.Bootstrap&quot;'.File.Data | base64 -d | jq -r . &gt; ign.json

cat .openshift_install_state.json | jq  -r '.&quot;*bootstrap.Bootstrap&quot;.Config.storage.files[].contents.source ' | sed 's/.*base64,//g' | base64 -d &gt; decode

cat .openshift_install_state.json | jq  -r '.&quot;*bootstrap.Bootstrap&quot;.Config.storage.files[] | .path, .contents.source ' | while read -r line ; do if [[ $line =~ .*base64,.* ]]; then echo $(echo $line | sed 's/.*base64,//g' | base64 -d) ; else echo $line; fi; done &gt; files

# https://serverfault.com/questions/329287/free-up-not-used-space-on-a-qcow2-image-file-on-kvm-qemu
virt-sparsify disk.img new-file.img



</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-49-静态ip-半离线-baremetal-安装-snosingle-node-openshift-模式"><a class="header" href="#openshift-49-静态ip-半离线-baremetal-安装-snosingle-node-openshift-模式">openshift 4.9 静态IP 半离线 baremetal 安装 SNO(single node openshift) 模式</a></h1>
<p>本文描述ocp4.9在baremetal(kvm模拟)上面，静态ip安装的方法。</p>
<p><img src="dia/4.9.bm.upi.sno.drawio.svg" alt="架构图" /></p>
<h2 id="离线安装包下载-1"><a class="header" href="#离线安装包下载-1">离线安装包下载</a></h2>
<p>打包好的安装包，在这里下载，百度盘下载链接，版本是 4.9.11 :</p>
<ul>
<li>链接: https://pan.baidu.com/s/1HxGdAp2ec4CZRVfeF8g35A 提取码: b4bl </li>
</ul>
<p>其中包括如下类型的文件：</p>
<ul>
<li>ocp4.tgz  这个文件包含了iso等安装介质，以及各种安装脚本，全部下载的镜像列表等。需要复制到宿主机，以及工具机上去。</li>
<li>registry.tgz  这个文件也是docker image registry的仓库打包文件。</li>
</ul>
<h2 id="宿主机准备-1"><a class="header" href="#宿主机准备-1">宿主机准备</a></h2>
<p>本次实验，是在一个32C， 256G 的主机上面，用很多个虚拟机安装测试。所以先准备这个宿主机。</p>
<p>如果是多台宿主机，记得一定要调整时间配置，让这些宿主机的时间基本一致，否则证书会出问题。</p>
<p>主要的准备工作有</p>
<ul>
<li>配置yum源</li>
<li>配置dns</li>
<li>安装镜像仓库</li>
<li>配置vnc环境</li>
<li>配置kvm需要的网络</li>
<li>创建helper kvm</li>
<li>配置一个haproxy，从外部导入流量给kvm</li>
</ul>
<p>以上准备工作，dns部分需要根据实际项目环境有所调整。</p>
<p>本次的宿主机是两台rocky linux</p>
<h2 id="kvm-host-101-1"><a class="header" href="#kvm-host-101-1">kvm host 101</a></h2>
<pre><code class="language-bash">
# 因为是半离线，我们的host os还有helper os是联线的，那么我们就用在线的源吧。
# dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm
dnf install -y epel-release

dnf install -y byobu htop dstat

# 准备vnc环境
vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/config
session=gnome
securitytypes=vncauth,tlsvnc
# desktop=sandbox
geometry=1280x800
alwaysshared
EOF

cat &lt;&lt; EOF &gt;&gt; /etc/tigervnc/vncserver.users
:1=root
EOF

# systemctl disable vncserver@:1
systemctl start vncserver@:1
# 如果你想停掉vnc server，这么做
systemctl stop vncserver@:1

# /usr/libexec/vncsession-start :1


# 配置kvm环境
dnf -y groupinstall &quot;Server with GUI&quot;

dnf -y install qemu-kvm libvirt libguestfs-tools virt-install virt-viewer virt-manager tigervnc-server

systemctl disable --now firewalld
systemctl enable --now libvirtd


# 创建实验用虚拟网络

mkdir -p /data/kvm
cat &lt;&lt; 'EOF' &gt; /data/kvm/bridge.sh
#!/usr/bin/env bash

PUB_CONN='eno1'
PUB_IP='172.21.6.104/24'
PUB_GW='172.21.6.254'
PUB_DNS='172.21.1.1'

nmcli con down &quot;$PUB_CONN&quot;
nmcli con delete &quot;$PUB_CONN&quot;
nmcli con down baremetal
nmcli con delete baremetal
# RHEL 8.1 appends the word &quot;System&quot; in front of the connection,delete in case it exists
nmcli con down &quot;System $PUB_CONN&quot;
nmcli con delete &quot;System $PUB_CONN&quot;
nmcli connection add ifname baremetal type bridge con-name baremetal ipv4.method 'manual' \
    ipv4.address &quot;$PUB_IP&quot; \
    ipv4.gateway &quot;$PUB_GW&quot; \
    ipv4.dns &quot;$PUB_DNS&quot;
    
nmcli con add type bridge-slave ifname &quot;$PUB_CONN&quot; master baremetal
nmcli con down &quot;$PUB_CONN&quot;;pkill dhclient;dhclient baremetal
nmcli con up baremetal
EOF
bash /data/kvm/bridge.sh

# 创建工具机

mkdir -p /data/kvm
cd /data/kvm

osinfo-query os | grep rhel8
#  rhel8-unknown        | Red Hat Enterprise Linux 8 Unknown                 | 8-unknown | http://redhat.com/rhel/8-unknown
#  rhel8.0              | Red Hat Enterprise Linux 8.0                       | 8.0      | http://redhat.com/rhel/8.0
#  rhel8.1              | Red Hat Enterprise Linux 8.1                       | 8.1      | http://redhat.com/rhel/8.1
#  rhel8.2              | Red Hat Enterprise Linux 8.2                       | 8.2      | http://redhat.com/rhel/8.2
#  rhel8.3              | Red Hat Enterprise Linux 8.3                       | 8.3      | http://redhat.com/rhel/8.3
#  rhel8.4              | Red Hat Enterprise Linux 8.4                       | 8.4      | http://redhat.com/rhel/8.4

wget https://mirrors.sjtug.sjtu.edu.cn/rocky/8.4/isos/x86_64/Rocky-8.4-x86_64-minimal.iso

# lvremove -f rhel/data
lvcreate -y -l 100%FREE -n data nvme
mkfs.xfs /dev/nvme/data
mkdir -p /data/nvme
mount /dev/nvme/data /data/nvme

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/nvme/data /data/nvme                   xfs     defaults        0 0
EOF

cd /data/kvm
wget https://mirrors.sjtug.sjtu.edu.cn/rocky/8.4/isos/x86_64/Rocky-8.4-x86_64-minimal.iso

export http_proxy=&quot;http://192.168.195.54:5085&quot;
export https_proxy=${http_proxy}

wget https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.8/scripts/helper-ks-rocky.cfg

unset http_proxy
unset https_proxy

sed -i '0,/^network.*/s/^network.*/network  --bootproto=static --device=enp1s0 --gateway=192.168.7.1 --ip=192.168.7.71  --netmask=255.255.255.0 --nameserver=192.168.7.71  --ipv6=auto --activate/' helper-ks-rhel8.cfg
# https://stackoverflow.com/questions/18620153/find-matching-text-and-replace-next-line
sed -i '/^network.*/{n;s/^network.*/network  --hostname=sno-helper/}' helper-ks-rhel8.cfg

export KVM_DIRECTORY=/home/data/kvm
virt-install --name=&quot;sno-aHelper&quot; --vcpus=2 --ram=4096 \
--cpu=host-model \
--disk path=${KVM_DIRECTORY}/sno-aHelper.qcow2,bus=virtio,size=20 \
--os-variant rhel8.3 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59200 \
--boot menu=on \
--location ${KVM_DIRECTORY}/rhel-8.3-x86_64-dvd.iso \
--disk ${KVM_DIRECTORY}/rhel-8.3-x86_64-dvd.iso,device=cdrom \
--initrd-inject helper-ks-rhel8.cfg --extra-args &quot;inst.ks=file:/helper-ks-rhel8.cfg&quot; 

# virt-viewer --domain-name ocp4-aHelper
# virsh start ocp4-aHelper
# virsh list --all

# start chrony/ntp server on host
# cat &lt;&lt; EOF &gt; /etc/chrony.conf
# driftfile /var/lib/chrony/drift
# makestep 1.0 3
# rtcsync
# allow 192.0.0.0/8
# local stratum 10
# logdir /var/log/chrony
# EOF

# echo &quot;allow 192.0.0.0/8&quot; &gt;&gt; /etc/chrony.conf
# systemctl enable --now chronyd
# # systemctl restart chronyd
# chronyc tracking
# chronyc sources -v
# chronyc sourcestats -v
# chronyc makestep

</code></pre>
<h2 id="工具机准备-1"><a class="header" href="#工具机准备-1">工具机准备</a></h2>
<p>以下是在工具机里面，进行的安装操作。</p>
<p>主要的操作有</p>
<ul>
<li>配置yum源</li>
<li>运行ansible脚本，自动配置工具机</li>
<li>上传定制的安装配置文件</li>
<li>生成ignition文件</li>
</ul>
<pre><code class="language-bash">
export YUMIP=&quot;192.168.7.1&quot;
cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote-ftp]
name=ftp
baseurl=ftp://${YUMIP}/
enabled=1
gpgcheck=0

EOF

# dnf install -y epel-release
# dnf install -y byobu
dnf update -y
reboot

sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config
systemctl restart sshd

cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

echo &quot;allow 192.0.0.0/8&quot; &gt;&gt; /etc/chrony.conf
systemctl enable --now chronyd
# systemctl restart chronyd
chronyc tracking
chronyc sources -v
chronyc sourcestats -v
chronyc makestep

# nmcli con mod enp1s0 +ipv4.addresses &quot;192.168.7.71/24&quot;
# nmcli con up enp1s0

dnf -y install ansible git unzip podman python3 buildah skopeo

mkdir -p /data/ocp4/
# scp ocp4.tgz to /data
# scp * root@172.21.6.11:/data/
cd /data
tar zvxf ocp.*.tgz
tar zvxf registry.*.tgz
cd /data/ocp4

rm -f /data/*.tgz

# 配置registry
# 配置registry
mkdir -p /etc/crts/ &amp;&amp; cd /etc/crts

# https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.1/html/installation_guide/installing-codeready-workspaces-in-tls-mode-with-self-signed-certificates_crw
openssl genrsa -out /etc/crts/redhat.ren.ca.key 4096
openssl req -x509 \
  -new -nodes \
  -key /etc/crts/redhat.ren.ca.key \
  -sha256 \
  -days 36500 \
  -out /etc/crts/redhat.ren.ca.crt \
  -subj /CN=&quot;Local Red Hat Ren Signer&quot; \
  -reqexts SAN \
  -extensions SAN \
  -config &lt;(cat /etc/pki/tls/openssl.cnf \
      &lt;(printf '[SAN]\nbasicConstraints=critical, CA:TRUE\nkeyUsage=keyCertSign, cRLSign, digitalSignature'))

openssl genrsa -out /etc/crts/redhat.ren.key 2048

openssl req -new -sha256 \
    -key /etc/crts/redhat.ren.key \
    -subj &quot;/O=Local Red Hat Ren /CN=*.ocp4.redhat.ren&quot; \
    -reqexts SAN \
    -config &lt;(cat /etc/pki/tls/openssl.cnf \
        &lt;(printf &quot;\n[SAN]\nsubjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;)) \
    -out /etc/crts/redhat.ren.csr

openssl x509 \
    -req \
    -sha256 \
    -extfile &lt;(printf &quot;subjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;) \
    -days 365 \
    -in /etc/crts/redhat.ren.csr \
    -CA /etc/crts/redhat.ren.ca.crt \
    -CAkey /etc/crts/redhat.ren.ca.key \
    -CAcreateserial -out /etc/crts/redhat.ren.crt

openssl x509 -in /etc/crts/redhat.ren.crt -text

/bin/cp -f /etc/crts/redhat.ren.ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

cd /data
# mkdir -p /data/registry
# tar zxf registry.tgz
dnf -y install podman pigz skopeo jq 
# pigz -dc registry.tgz | tar xf -
cd /data/ocp4
podman load -i /data/ocp4/registry.tgz

systemctl disable --now firewalld

podman run --restart always --name local-registry -p 5443:5443 \
  -d --restart=always \
  -v /home/ocp.4.9.5/registry/:/var/lib/registry:z \
  -v /etc/crts:/certs:z \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:5443 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/redhat.ren.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/redhat.ren.key \
  docker.io/library/registry:2

podman start local-registry

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/building_running_and_managing_containers/assembly_porting-containers-to-systemd-using-podman_building-running-and-managing-containers

# podman generate systemd --new --files --name local-registry
podman generate systemd --files --name local-registry
# /root/container-local-registry.service
cp -Z container-local-registry.service  /usr/lib/systemd/system

systemctl enable --now container-local-registry.service
systemctl status container-local-registry.service

# podman rm --storage 7cb9fcea76ad384313a682a469be6784786eb5004a190ad2abe68978b1566416

# firewall-cmd --permanent --add-port=5443/tcp
# firewall-cmd --reload

# 加载更多的镜像
# 解压缩 ocp4.tgz
# bash add.image.load.sh /data/4.6.5/install.image 'registry.ocp4.redhat.ren:5443'

# https://github.com/christianh814/ocp4-upi-helpernode/blob/master/docs/quickstart.md

# in helper node
# mkdir /etc/yum.repos.d.bak
# mv /etc/yum.repos.d/* /etc/yum.repos.d.bak/
# cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
# [remote]
# name=RHEL FTP
# baseurl=ftp://192.168.7.1/data
# enabled=1
# gpgcheck=0

# EOF

# yum clean all
# yum repolist

# 这里使用了一个ansible的项目，用来部署helper节点的服务。
# https://github.com/wangzheng422/ocp4-upi-helpernode
cd /data/ocp4
unzip ocp4-upi-helpernode.zip
# 这里使用了一个ignition文件合并的项目，用来帮助自定义ignition文件。
# https://github.com/wangzheng422/filetranspiler
# podman load -i filetranspiler.tgz

# podman load -i nexus.tgz

ssh-keygen

# 接下来，我们使用ansible来配置helper节点，装上各种openshift集群需要的服务
# 根据现场环境，修改 ocp4-upi-helpernode-master/vars-static.yaml
# 主要是修改各个节点的网卡和硬盘参数，还有IP地址
cd /data/ocp4/ocp4-upi-helpernode-master
# ansible-playbook -e @vars-static.yaml -e '{staticips: true}' tasks/main.yml

cat &lt;&lt; 'EOF' &gt; /data/ocp4/ocp4-upi-helpernode-master/vars.yaml
---
ocp_version: 4.9.5
ssh_gen_key: false
staticips: true
firewalld: false
dns_forward: yes
iso:
  iso_dl_url: &quot;/data/ocp4/rhcos-live.x86_64.iso&quot;
  my_iso: &quot;rhcos-live.iso&quot; # this is internal file, just leave as it.
helper:
  name: &quot;helper&quot;
  ipaddr: &quot;192.168.7.71&quot;
  networkifacename: &quot;enp1s0&quot;
  gateway: &quot;192.168.7.1&quot;
  netmask: &quot;255.255.255.0&quot;
dns:
  domain: &quot;redhat.ren&quot;
  clusterid: &quot;ocp4s&quot;
  forwarder1: &quot;202.106.0.20&quot;
  forwarder2: &quot;202.106.0.20&quot;
bootstrap:
  name: &quot;bootstrap&quot;
  ipaddr: &quot;192.168.7.72&quot;
  interface: &quot;enp1s0&quot;
  install_drive: &quot;vda&quot;
masters:
  - name: &quot;master-0&quot;
    ipaddr: &quot;192.168.7.73&quot;
    interface: &quot;enp103s0f1&quot;
    install_drive: &quot;sda&quot;
    disable_interfaces:
      - interface: &quot;enp3s0&quot;
      - interface: &quot;enp103s0f0&quot;
  # - name: &quot;master-1&quot;
  #   ipaddr: &quot;192.168.7.14&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;    
  # - name: &quot;master-2&quot;
  #   ipaddr: &quot;192.168.7.15&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;    
# workers:
  # - name: &quot;worker-0&quot;
  #   ipaddr: &quot;192.168.7.16&quot;
  #   interface: &quot;ens3f0&quot;
  #   install_drive: &quot;sda&quot;
  # - name: &quot;worker-1&quot;
  #   ipaddr: &quot;192.168.7.17&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;sda&quot;
  # - name: &quot;worker-2&quot;
  #   ipaddr: &quot;192.168.7.18&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
  # - name: &quot;infra-0&quot;
  #   ipaddr: &quot;192.168.7.19&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
  # - name: &quot;infra-1&quot;
  #   ipaddr: &quot;192.168.7.20&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
  # - name: &quot;worker-3&quot;
  #   ipaddr: &quot;192.168.7.21&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
  # - name: &quot;worker-4&quot;
  #   ipaddr: &quot;192.168.7.22&quot;
  #   interface: &quot;enp1s0&quot;
  #   install_drive: &quot;vda&quot;
others:
  - name: &quot;registry&quot;
    ipaddr: &quot;192.168.7.1&quot;
  - name: &quot;yum&quot;
    ipaddr: &quot;192.168.7.1&quot;
  - name: &quot;quay&quot;
    ipaddr: &quot;192.168.7.11&quot;
  - name: &quot;nexus&quot;
    ipaddr: &quot;192.168.7.1&quot;
  - name: &quot;git&quot;
    ipaddr: &quot;192.168.7.11&quot;
otherdomains:
  - domain: &quot;rhv.redhat.ren&quot;
    hosts:
    - name: &quot;manager&quot;
      ipaddr: &quot;192.168.7.71&quot;
    - name: &quot;rhv01&quot;
      ipaddr: &quot;192.168.7.72&quot;
  - domain: &quot;others.redhat.ren&quot;
    hosts:
    - name: &quot;*&quot;
      ipaddr: &quot;192.168.7.71&quot;
    - name: &quot;*.apps&quot;
      ipaddr: &quot;192.168.7.71&quot;
  - domain: &quot;ocp4.redhat.ren&quot;
    hosts:
      - name: &quot;registry&quot;
        ipaddr: &quot;192.168.7.1&quot;
      - name: &quot;yum&quot;
        ipaddr: &quot;192.168.7.1&quot;
      - name: &quot;quay&quot;
        ipaddr: &quot;192.168.7.11&quot;
      - name: &quot;nexus&quot;
        ipaddr: &quot;192.168.7.1&quot;
      - name: &quot;git&quot;
        ipaddr: &quot;192.168.7.11&quot;
force_ocp_download: false
remove_old_config_files: false
ocp_client: &quot;file:///data/ocp4/{{ ocp_version }}/openshift-client-linux-{{ ocp_version }}.tar.gz&quot;
ocp_installer: &quot;file:///data/ocp4/{{ ocp_version }}/openshift-install-linux-{{ ocp_version }}.tar.gz&quot;
ppc64le: false
arch: 'x86_64'
chronyconfig:
  enabled: true
  content:
    - server: &quot;192.168.7.1&quot;
      options: iburst
setup_registry: # don't worry about this, just leave it here
  deploy: false
  registry_image: docker.io/library/registry:2
  local_repo: &quot;ocp4/openshift4&quot;
  product_repo: &quot;openshift-release-dev&quot;
  release_name: &quot;ocp-release&quot;
  release_tag: &quot;4.6.1-x86_64&quot;
ocp_filetranspiler: &quot;file:///data/ocp4/filetranspiler.tgz&quot;
registry_server: &quot;registry.ocp4.redhat.ren:5443&quot;
EOF

ansible-playbook -e @vars.yaml tasks/main.yml

# try this:
/usr/local/bin/helpernodecheck

mkdir -p /data/install

# # GOTO image registry host
# # copy crt files to helper node
# scp /etc/crts/redhat.ren.ca.crt root@192.168.7.11:/data/install/
# scp /etc/crts/redhat.ren.crt root@192.168.7.11:/data/install/
# scp /etc/crts/redhat.ren.key root@192.168.7.11:/data/install/

# GO back to help node
# /bin/cp -f /data/install/redhat.ren.crt /etc/pki/ca-trust/source/anchors/
# update-ca-trust extract

# 定制ignition
cd /data/install

# 根据现场环境，修改 install-config.yaml
# 至少要修改ssh key， 还有 additionalTrustBundle，这个是镜像仓库的csr 

# vi install-config.yaml 
cat &lt;&lt; EOF &gt; /data/install/install-config.yaml 
apiVersion: v1
baseDomain: redhat.ren
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 0
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 1
metadata:
  name: ocp4s
networking:
  clusterNetworks:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
pullSecret: '{&quot;auths&quot;:{&quot;registry.ocp4.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;},&quot;registry.ppa.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;}}}'
sshKey: |
$( cat /root/.ssh/id_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /etc/crts/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  - registry.ocp4.redhat.ren:5443/ocp4/release
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  - registry.ocp4.redhat.ren:5443/ocp4/release
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

cd /data/install/
/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] 

openshift-install create manifests --dir=/data/install

# copy ntp related config
/bin/cp -f  /data/ocp4/ocp4-upi-helpernode-master/machineconfig/* /data/install/openshift/

# copy image registry proxy related config
cd /data/ocp4
bash image.registries.conf.sh nexus.ocp4.redhat.ren:8083

/bin/cp -f /data/ocp4/image.registries.conf /etc/containers/registries.conf.d/

/bin/cp -f /data/ocp4/99-worker-container-registries.yaml /data/install/openshift
/bin/cp -f /data/ocp4/99-master-container-registries.yaml /data/install/openshift

cd /data/install/
openshift-install create ignition-configs --dir=/data/install

cd /data/ocp4/ocp4-upi-helpernode-master
# 我们来为每个主机，复制自己版本的ign，并复制到web server的目录下
ansible-playbook -e @vars.yaml tasks/ign.yml
# 如果对每个主机有自己ign的独特需求，在这一步，去修改ign。

# 以下操作本来是想设置网卡地址，但是实践发现是不需要的。
# 保留在这里，是因为他可以在安装的时候注入文件，非常有用。
# mkdir -p bootstrap/etc/sysconfig/network-scripts/
# cat &lt;&lt;EOF &gt; bootstrap/etc/sysconfig/network-scripts/ifcfg-ens3
# DEVICE=ens3
# BOOTPROTO=none
# ONBOOT=yes
# IPADDR=192.168.7.12
# NETMASK=255.255.255.0
# GATEWAY=192.168.7.1
# DNS=192.168.7.11
# DNS1=192.168.7.11
# DNS2=192.168.7.1
# DOMAIN=redhat.ren
# PREFIX=24
# DEFROUTE=yes
# IPV6INIT=no
# EOF
# filetranspiler -i bootstrap.ign -f bootstrap -o bootstrap-static.ign
# /bin/cp -f bootstrap-static.ign /var/www/html/ignition/

# 我们为每个节点创建各自的iso文件
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars.yaml tasks/iso.yml

# if boot using live-iso, you need to run this cmd during install
nmtui
coreos-installer install --copy-network \
     --ignition-url=http://192.168.7.71:8080/ignition/master-0.ign /dev/sda --insecure-ignition

</code></pre>
<h2 id="回到宿主机-1"><a class="header" href="#回到宿主机-1">回到宿主机</a></h2>
<p>本来，到了这一步，就可以开始安装了，但是我们知道coreos装的时候，要手动输入很长的命令行，实际操作的时候，那是不可能输入对的，输入错一个字符，安装就失败，要重启，重新输入。。。</p>
<p>为了避免这种繁琐的操作，参考网上的做法，我们就需要为每个主机定制iso了。好在，之前的步骤，我们已经用ansible创建了需要的iso，我们把这些iso复制到宿主机上，就可以继续了。</p>
<p>这里面有一个坑，我们是不知道主机的网卡名称的，只能先用coreos iso安装启动一次，进入单用户模式以后，ip a 来查看以下，才能知道，一般来说，是ens3。</p>
<p>另外，如果是安装物理机，disk是哪个，也需要上述的方法，来看看具体的盘符。另外，推荐在物理机上安装rhel 8 来测试一下物理机是不是支持coreos。物理机安装的时候，遇到不写盘的问题，可以尝试添加启动参数： ignition.firstboot=1</p>
<pre><code class="language-bash"># on kvm host 172.21.6.101
export KVM_DIRECTORY=/home/data/kvm

mkdir -p  ${KVM_DIRECTORY}
cd ${KVM_DIRECTORY}
scp root@192.168.7.71:/data/install/{*boot*,*master-0,*worker-0}.iso ${KVM_DIRECTORY}/

# on kvm host 172.21.6.101

# finally, we can start install :)
# 你可以一口气把虚拟机都创建了，然后喝咖啡等着。
# 从这一步开始，到安装完毕，大概30分钟。
# export KVM_DIRECTORY=/data/kvm
virt-install --name=sno-bootstrap --vcpus=4 --ram=8192 \
--disk path=${KVM_DIRECTORY}/ocp4-bootstrap.qcow2,bus=virtio,size=50 \
--os-variant rhel8.3 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59101 \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-bootstrap.iso   

# 想登录进coreos一探究竟？那么这么做
# ssh core@bootstrap 
# journalctl -b -f -u bootkube.service
# export KVM_DIRECTORY=/data/kvm
virt-install --name=sno-master-0 --vcpus=16 --ram=49152 \
--cpu=host-model \
--disk path=${KVM_DIRECTORY}/ocp4-master-0.qcow2,bus=virtio,size=120 \
--os-variant rhel8.3 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59002 \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-0.iso 

# virt-install --name=ocp4-master-1 --vcpus=10 --ram=20480 \
# --cpu=host-model \
# --disk path=/data/nvme/ocp4-master-1.qcow2,bus=virtio,size=120 \
# --os-variant rhel8.4 --network bridge=baremetal,model=virtio \
# --graphics vnc,port=59003 \
# --boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-1.iso 

# # ssh core@192.168.7.13

# # on kvm host 172.21.6.103
# export KVM_DIRECTORY=/data/kvm

# virt-install --name=ocp4-master-2 --vcpus=22 --ram=30720 \
# --cpu=host-model \
# --disk path=/data/kvm/ocp4-master-2.qcow2,bus=virtio,size=120 \
# --os-variant rhel8.4 --network bridge=baremetal,model=virtio \
# --graphics vnc,port=59004 \
# --boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-2.iso 

# on kvm host 172.21.6.104
export KVM_DIRECTORY=/data/kvm

# virt-install --name=ocp4-worker-0 --vcpus=4 --ram=10240 \
# --disk path=/data/kvm/ocp4-worker-0.qcow2,bus=virtio,size=120 \
# --os-variant rhel8.4 --network bridge=baremetal,model=virtio \
# --graphics vnc,port=59005 \
# --boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-worker-0.iso 

# if install on baremetal
nmtui
coreos-install install --copy-network --ignition-url=http://192.168.7.71:8080/ignition/master-0.ign --inscure-ignition /dev/sda 

# on workstation
# open http://192.168.7.11:9000/
# to check

# if you want to stop or delete vm, try this
virsh list --all
virsh destroy ocp4-bootstrap
virsh destroy ocp4-master-0 
# virsh destroy ocp4-master-1 
# virsh destroy ocp4-master-2 
# virsh destroy ocp4-worker0 
# virsh destroy ocp4-worker1 
# virsh destroy ocp4-worker2
virsh undefine ocp4-bootstrap --remove-all-storage
virsh undefine ocp4-master-0 --remove-all-storage
# virsh undefine ocp4-master-1 --remove-all-storage
# virsh undefine ocp4-master-2 --remove-all-storage
# virsh undefine ocp4-worker0 
# virsh undefine ocp4-worker1 
# virsh undefine ocp4-worker2

</code></pre>
<h2 id="在工具机上面-1"><a class="header" href="#在工具机上面-1">在工具机上面</a></h2>
<p>这个时候，安装已经自动开始了，我们只需要回到工具机上静静的观察就可以了。</p>
<p>在bootstrap和装master阶段，用这个命令看进度。</p>
<pre><code class="language-bash">cd /data/install
export KUBECONFIG=/data/install/auth/kubeconfig
echo &quot;export KUBECONFIG=/data/install/auth/kubeconfig&quot; &gt;&gt; ~/.bashrc
oc completion bash | sudo tee /etc/bash_completion.d/openshift &gt; /dev/null

cd /data/install
openshift-install wait-for bootstrap-complete --log-level debug

</code></pre>
<p>有时候证书会过期，验证方法是登录 bootstrap, 看看过期时间。如果确定过期，要清除所有的openshift-install生成配置文件的缓存，重新来过。</p>
<pre><code class="language-bash">echo | openssl s_client -connect localhost:6443 | openssl x509 -noout -text | grep Not
</code></pre>
<p>一般来说，如果在openshift-install这一步之前，按照文档，删除了缓存文件，就不会出现过期的现象。</p>
<pre><code class="language-bash">oc get nodes
</code></pre>
<p>这个时候，只能看到master，是因为worker的csr没有批准。如果虚拟机是一口气创建的，那么多半不会遇到下面的问题。</p>
<pre><code class="language-bash">oc get csr
</code></pre>
<p>会发现有很多没有被批准的</p>
<p>批准之</p>
<pre><code class="language-bash">yum -y install jq
oc get csr | grep -v Approved
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve
# oc get csr -o name | xargs oc adm certificate approve
</code></pre>
<p>然后worker 节点cpu飙高，之后就能看到worker了。</p>
<p>上面的操作完成以后，就可以完成最后的安装了</p>
<pre><code class="language-bash">openshift-install wait-for install-complete --log-level debug
# here is the output
INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4s.redhat.ren
# INFO Login to the console with user: &quot;kubeadmin&quot;, and password: &quot;4RtR2-nITmZ-hTP6g-Q2iHw&quot;

# we are testing env, so we don't need ingress replicas.
oc patch --namespace=openshift-ingress-operator --patch='{&quot;spec&quot;: {&quot;replicas&quot;: 1}}' --type=merge ingresscontroller/default

# after install finish, delete bootstrap vm,
# and we need to fix the dns setting, 
# remove them from helper to master-0
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars.yaml tasks/sno.dns.yml


</code></pre>
<h2 id="镜像仓库代理--image-registry-proxy-1"><a class="header" href="#镜像仓库代理--image-registry-proxy-1">镜像仓库代理 / image registry proxy</a></h2>
<p>准备离线镜像仓库非常麻烦，好在我们找到了一台在线的主机，那么我们可以使用nexus构造image registry proxy，在在线环境上面，做一遍PoC，然后就能通过image registry proxy得到离线镜像了</p>
<ul>
<li>https://mtijhof.wordpress.com/2018/07/23/using-nexus-oss-as-a-proxy-cache-for-docker-images/</li>
</ul>
<h2 id="配置镜像仓库的ca-1"><a class="header" href="#配置镜像仓库的ca-1">配置镜像仓库的ca</a></h2>
<p>安装过程里面，已经把镜像仓库的ca放进去了，但是好想image stream不认，让我们再试试</p>
<pre><code class="language-bash">oc project openshift-config
oc create configmap ca.for.registry -n openshift-config \
    --from-file=registry.ocp4.redhat.ren..5443=/etc/crts/redhat.ren.ca.crt 
#     --from-file=nexus.ocp4.redhat.ren..8083=/data/install/redhat.ren.ca.crt 

oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;additionalTrustedCA&quot;:{&quot;name&quot;:&quot;ca.for.registry&quot;}}}'  --type=merge

oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;registrySources&quot;:{&quot;insecureRegistries&quot;:[&quot;nexus.ocp4.redhat.ren:8083&quot;]}}}'  --type=merge

oc get image.config.openshift.io/cluster -o yaml

# openshift project下面的image stream重新加载一下把
oc get is -o json | jq -r '.items[].metadata.name' | xargs -L1 oc import-image --all 

</code></pre>
<h2 id="配置internal-registry-1"><a class="header" href="#配置internal-registry-1">配置internal registry</a></h2>
<p>我们的工具机是带nfs的，那么就给interneal registry配置高档一些的nfs存储吧，不要用emptydir</p>
<pre><code class="language-bash">bash /data/ocp4/ocp4-upi-helpernode-master/files/nfs-provisioner-setup.sh

# oc edit configs.imageregistry.operator.openshift.io
# 修改 storage 部分
# storage:
#   pvc:
#     claim:
oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;,&quot;storage&quot;:{&quot;pvc&quot;:{&quot;claim&quot;:&quot;&quot;}}}}' --type=merge

# if you want to restore
oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

oc get clusteroperator image-registry

oc get configs.imageregistry.operator.openshift.io cluster -o yaml

# 把imagepruner给停掉
# https://bugzilla.redhat.com/show_bug.cgi?id=1852501#c24
# oc patch imagepruner.imageregistry/cluster --patch '{&quot;spec&quot;:{&quot;suspend&quot;:true}}' --type=merge
# oc -n openshift-image-registry delete jobs --all
</code></pre>
<h2 id="配置sample-operator-1"><a class="header" href="#配置sample-operator-1">配置sample operator</a></h2>
<p>openshift内置了一个sample operator，里面有一大堆红帽的产品。</p>
<pre><code class="language-bash">oc get configs.samples.operator.openshift.io/cluster -o yaml

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;, &quot;samplesRegistry&quot;: &quot;nexus.ocp4.redhat.ren:8083&quot;}}' --type=merge

# if you want to restore
oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Unmanaged&quot;}}' --type=merge

# if you want to get ride of sampe operator
oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

</code></pre>
<h2 id="chronyntp-设置-1"><a class="header" href="#chronyntp-设置-1">chrony/NTP 设置</a></h2>
<p>在 ocp 4.6 里面，需要设定ntp同步，我们之前ansible脚本，已经创建好了ntp的mco配置，把他打到系统里面就好了。</p>
<pre><code class="language-bash">oc apply -f /data/ocp4/ocp4-upi-helpernode-master/machineconfig/

</code></pre>
<h2 id="operator-hub-离线安装-1"><a class="header" href="#operator-hub-离线安装-1">Operator Hub 离线安装</a></h2>
<p>使用nexus作为image proxy以后，就不需要做这个离线操作了。有些情况下，我们可能还需要屏蔽掉默认的operator hub</p>
<pre><code class="language-bash">
oc patch OperatorHub cluster --type json \
    -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true}]'

oc get OperatorHub cluster -o yaml

oc get OperatorHub
# NAME      AGE
# cluster   20h
</code></pre>
<h2 id="给-openshift-project-image-stream-打补丁-1"><a class="header" href="#给-openshift-project-image-stream-打补丁-1">给 openshift project image stream 打补丁</a></h2>
<p>在有代理的网络环境中，我们需要给openshift project下的image stream打一些补丁。</p>
<pre><code class="language-bash">cd /data/ocp4
bash is.patch.sh registry.ocp4.redhat.ren:5443/ocp4/openshift4

</code></pre>
<h2 id="disable-offical-helm-chart--enable-helm-proxy-1"><a class="header" href="#disable-offical-helm-chart--enable-helm-proxy-1">disable offical helm chart &amp; enable helm proxy</a></h2>
<p>我们是半离线环境，所以openshift4内置的官方helm chart是无法访问的，我们禁用之。</p>
<pre><code class="language-bash">oc get HelmChartRepository
# NAME               AGE
# redhat-helm-repo   19h

# oc patch HelmChartRepository redhat-helm-repo --type json \
#     -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disabled&quot;, &quot;value&quot;: true}]'

oc patch  --patch='{&quot;spec&quot;: {&quot;disabled&quot;: true}}' --type=merge HelmChartRepository/openshift-helm-charts

cat &lt;&lt; EOF &gt; /data/install/helm.ocp.yaml
apiVersion: helm.openshift.io/v1beta1
kind: HelmChartRepository
metadata:
  name: openshift-helm-charts-wzh
spec:
 # optional name that might be used by console
  name: openshift-helm-charts-wzh
  connectionConfig:
    url: http://nexus.ocp4.redhat.ren:8082/repository/charts.openshift.io/
EOF
oc create -f /data/install/helm.ocp.yaml

</code></pre>
<h2 id="给-router--ingress-更换证书-1"><a class="header" href="#给-router--ingress-更换证书-1">给 router / ingress 更换证书</a></h2>
<p>有时候，我们需要公网CA认证的证书，给router来用，那么我们就搞一下</p>
<p>https://docs.openshift.com/container-platform/4.6/security/certificates/replacing-default-ingress-certificate.html</p>
<pre><code class="language-bash">
mkdir -p /data/ccn/ingress-keys/etc
mkdir -p /data/ccn/ingress-keys/lib
cd /data/ccn/ingress-keys
podman run -it --rm --name certbot \
            -v &quot;/data/ccn/ingress-keys/etc:/etc/letsencrypt&quot;:Z \
            -v &quot;/data/ccn/ingress-keys/lib:/var/lib/letsencrypt&quot;:Z \
            docker.io/certbot/certbot certonly  -d &quot;*.apps.ocp4.redhat.ren&quot; --manual --preferred-challenges dns-01  --server https://acme-v02.api.letsencrypt.org/directory

cp ./etc/archive/apps.ocp4.redhat.ren/fullchain1.pem apps.ocp4.redhat.ren.crt
cp ./etc/archive/apps.ocp4.redhat.ren/privkey1.pem apps.ocp4.redhat.ren.key

ssh root@192.168.7.11 mkdir -p /data/install/ingress-key

scp apps.* root@192.168.7.11:/data/install/ingress-key

# on helper
cd /data/install/ingress-key

oc create secret tls wzh-ingress-key \
     --cert=apps.ocp4.redhat.ren.crt \
     --key=apps.ocp4.redhat.ren.key \
     -n openshift-ingress

oc patch ingresscontroller.operator default \
     --type=merge -p \
     '{&quot;spec&quot;:{&quot;defaultCertificate&quot;: {&quot;name&quot;: &quot;wzh-ingress-key&quot;}}}' \
     -n openshift-ingress-operator

</code></pre>
<h2 id="更改系统默认时区-1"><a class="header" href="#更改系统默认时区-1">更改系统默认时区</a></h2>
<p>https://access.redhat.com/solutions/5487331</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /data/install/timezone.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: worker-custom-timezone-configuration
spec:
  config:
    ignition:
      version: 2.2.0
    systemd:
      units:
      - contents: |
          [Unit]
          Description=set timezone
          After=network-online.target

          [Service]
          Type=oneshot
          ExecStart=timedatectl set-timezone Asia/Shanghai

          [Install]
          WantedBy=multi-user.target
        enabled: true
        name: custom-timezone.service
EOF
oc create -f /data/install/timezone.yaml

</code></pre>
<h2 id="排错技巧-1"><a class="header" href="#排错技巧-1">排错技巧</a></h2>
<pre><code class="language-bash">
# login to bootstrap to debug
# find the ip from kvm console
ssh -i ~/.ssh/helper_rsa core@192.168.7.75
journalctl -b -f -u release-image.service -u bootkube.service
journalctl -b -u release-image.service -u bootkube.service | grep -i baremetal
sudo -i
export KUBECONFIG=/etc/kubernetes/kubeconfig
oc get pod -n openshift-machine-api
oc get BareMetalHost -n openshift-machine-api

# debug why bootstrap can't be ping...
cat .openshift_install_state.json | jq  '.&quot;*bootstrap.Bootstrap&quot;'.Config.storage.files[].path

cat .openshift_install_state.json | jq -r '.&quot;*bootstrap.Bootstrap&quot;'.File.Data | base64 -d | jq -r . &gt; ign.json

cat .openshift_install_state.json | jq  -r '.&quot;*bootstrap.Bootstrap&quot;.Config.storage.files[].contents.source ' | sed 's/.*base64,//g' | base64 -d &gt; decode

cat .openshift_install_state.json | jq  -r '.&quot;*bootstrap.Bootstrap&quot;.Config.storage.files[] | .path, .contents.source ' | while read -r line ; do if [[ $line =~ .*base64,.* ]]; then echo $(echo $line | sed 's/.*base64,//g' | base64 -d) ; else echo $line; fi; done &gt; files

# https://serverfault.com/questions/329287/free-up-not-used-space-on-a-qcow2-image-file-on-kvm-qemu
virt-sparsify disk.img new-file.img

# https://access.redhat.com/solutions/24029
xz -dc &lt; /boot/initrd-$(uname -r).img | cpio -idmv

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="real-time-kernel-for-openshift49"><a class="header" href="#real-time-kernel-for-openshift49">Real-Time Kernel for Openshift4.9</a></h1>
<p>本次试验部署架构图</p>
<p><img src="dia/4.7.real-time.kernel.drawio.svg" alt="" /></p>
<h1 id="硬件部署"><a class="header" href="#硬件部署">硬件部署</a></h1>
<p>假设有专属的FPGA，每个FPGA可以连4个RRU，另外GPS连接也是在FPGA上。</p>
<h1 id="先使用performance-addon-operator这个是官方推荐的方法"><a class="header" href="#先使用performance-addon-operator这个是官方推荐的方法">先使用performance addon operator，这个是官方推荐的方法。</a></h1>
<p>performance addon operator 是openshift4里面的一个operator，他的作用是，让用户进行简单的yaml配置，然后operator帮助客户进行复杂的kernel parameter, kubelet, tuned配置。</p>
<pre><code class="language-bash">
# install performance addon operator following offical document
# https://docs.openshift.com/container-platform/4.9/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html

cat &lt;&lt; EOF &gt; /data/install/pao-namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-performance-addon-operator
  annotations:
    workload.openshift.io/allowed: management
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-performance-addon-operator
  namespace: openshift-performance-addon-operator
---
EOF
oc create -f /data/install/pao-namespace.yaml

# then install pao in project openshift-performance-addon-operator

# then create mcp, be careful, the label must be there
cat &lt;&lt; EOF &gt; /data/install/worker-rt.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-rt
  labels:
    machineconfiguration.openshift.io/role: worker-rt
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-rt]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-rt: &quot;&quot;

EOF
oc create -f /data/install/worker-rt.yaml

# to restore
oc delete -f /data/install/worker-rt.yaml

oc label node worker-0 node-role.kubernetes.io/worker-rt=&quot;&quot;

# 以下的配置，是保留了0-1核给系统，剩下的2-19核给应用。
cat &lt;&lt; EOF &gt; /data/install/performance.yaml
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
   name: wzh-performanceprofile
spec:
  additionalKernelArgs:
    - no_timer_check
    - clocksource=tsc
    - tsc=perfect
    - selinux=0
    - enforcing=0
    - nmi_watchdog=0
    - softlockup_panic=0
    - isolcpus=2-19
    - nohz_full=2-19
    - idle=poll
    - default_hugepagesz=1G
    - hugepagesz=1G
    - hugepages=16
    - skew_tick=1
    - rcu_nocbs=2-19
    - kthread_cpus=0-1
    - irqaffinity=0-1
    - rcu_nocb_poll
    - iommu=pt
    - intel_iommu=on
    # profile creator
    - audit=0
    - idle=poll
    - intel_idle.max_cstate=0
    - mce=off
    - nmi_watchdog=0
    - nosmt
    - processor.max_cstate=1
  globallyDisableIrqLoadBalancing: true
  cpu:
      isolated: &quot;2-19&quot;
      reserved: &quot;0-1&quot;
  realTimeKernel:
      enabled: true
  numa:  
      topologyPolicy: &quot;single-numa-node&quot;
  nodeSelector:
      node-role.kubernetes.io/worker-rt: &quot;&quot;
  machineConfigPoolSelector:
    machineconfiguration.openshift.io/role: worker-rt
EOF
oc create -f /data/install/performance.yaml

# it will create following
  # runtimeClass: performance-wzh-performanceprofile
  # tuned: &gt;-
  #   openshift-cluster-node-tuning-operator/openshift-node-performance-wzh-performanceprofile

oc get mc/50-nto-worker-rt -o yaml
oc get runtimeClass -o yaml
oc get -n openshift-cluster-node-tuning-operator tuned/openshift-node-performance-wzh-performanceprofile -o yaml

# restore
oc delete -f /data/install/performance.yaml

# enable sctp
# https://docs.openshift.com/container-platform/4.9/networking/using-sctp.html
cat &lt;&lt; EOF &gt; /data/install/sctp-module.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 99-worker-rt-load-sctp-module
  labels:
    machineconfiguration.openshift.io/role: worker-rt
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modprobe.d/sctp-blacklist.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,
        - path: /etc/modules-load.d/sctp-load.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,sctp
EOF
oc create -f /data/install/sctp-module.yaml

# check the result
ssh core@worker-0
uname -a
# Linux worker-0 4.18.0-193.51.1.rt13.101.el8_2.x86_64 #1 SMP PREEMPT RT Thu Apr 8 17:21:44 EDT 2021 x86_64 x86_64 x86_64 GNU/Linux

ps -ef | grep stalld
# root        4416       1  0 14:04 ?        00:00:00 /usr/local/bin/stalld -p 1000000000 -r 10000 -d 3 -t 20 --log_syslog --log_kmsg --foreground --pidfile /run/stalld.pid
# core        6601    6478  0 14:08 pts/0    00:00:00 grep --color=auto stalld

</code></pre>
<h1 id="create-demo-vbbu-app"><a class="header" href="#create-demo-vbbu-app">create demo vBBU app</a></h1>
<pre><code class="language-yaml">---

apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: host-device-du
spec:
  config: '{
    &quot;cniVersion&quot;: &quot;0.3.0&quot;,
    &quot;type&quot;: &quot;host-device&quot;,
    &quot;device&quot;: &quot;ens18f1&quot;,
    &quot;ipam&quot;: {
      &quot;type&quot;: &quot;host-local&quot;,
      &quot;subnet&quot;: &quot;192.168.12.0/24&quot;,
      &quot;rangeStart&quot;: &quot;192.168.12.105&quot;,
      &quot;rangeEnd&quot;: &quot;192.168.12.106&quot;,
      &quot;routes&quot;: [{
        &quot;dst&quot;: &quot;0.0.0.0/0&quot;
      }],
      &quot;gateway&quot;: &quot;192.168.12.1&quot;
    }
  }'

# apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
# kind: NetworkAttachmentDefinition
# metadata:
#   name: host-device-du
# spec:
#   config: '{
#     &quot;cniVersion&quot;: &quot;0.3.0&quot;,
#     &quot;type&quot;: &quot;host-device&quot;,
#     &quot;device&quot;: &quot;ens18f1&quot;,
#     &quot;ipam&quot;: {
#       &quot;type&quot;: &quot;static&quot;,
#       &quot;addresses&quot;: [ 
#             {
#               &quot;address&quot;: &quot;192.168.12.105/24&quot;
#             },
#             {
#               &quot;address&quot;: &quot;192.168.12.106/24&quot;
#             }
#           ],
#       &quot;routes&quot;: [ 
#           {
#             &quot;dst&quot;: &quot;0.0.0.0/0&quot;, 
#             &quot;gw&quot;: &quot;192.168.12.1&quot;
#           }
#         ]
#       }
#     }'


---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: du-deployment1
  labels:
    app: du-deployment1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: du-pod1
  template:
    metadata:
      labels:
        app: du-pod1
      annotations:
        k8s.v1.cni.cncf.io/networks: '[
          { &quot;name&quot;: &quot;host-device-du&quot;,
            &quot;interface&quot;: &quot;veth11&quot; }
          ]'
      cpu-load-balancing.crio.io: &quot;true&quot;
    spec:
      runtimeClassName: performance-wzh-performanceprofile
      containers:
      - name: du-container1
        image: &quot;registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh-shell-03&quot;
        imagePullPolicy: IfNotPresent
        tty: true
        stdin: true
        env:
          - name: duNetProviderDriver
            value: &quot;host-netdevice&quot;
        #command:
        #  - sleep
        #  - infinity
        securityContext:
            privileged: true
            capabilities:
                add:
                - CAP_SYS_ADMIN
        volumeMounts:
          - mountPath: /hugepages
            name: hugepage
          - name: lib-modules
            mountPath: /lib/modules
          - name: src
            mountPath: /usr/src
          - name: dev
            mountPath: /dev
          - name: cache-volume
            mountPath: /dev/shm
        resources:
          requests:
            cpu: 16
            memory: 48Gi
            hugepages-1Gi: 8Gi
          limits:
            cpu: 16
            memory: 48Gi
            hugepages-1Gi: 8Gi
      volumes:
        - name: hugepage
          emptyDir:
            medium: HugePages
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: src
          hostPath:
            path: /usr/src
        - name: dev
          hostPath:
            path: &quot;/dev&quot;
        - name: cache-volume
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      nodeSelector:
        node-role.kubernetes.io/worker-rt: &quot;&quot;

---

# apiVersion: v1
# kind: Service
# metadata:
#   name: du-http 
# spec:
#   ports:
#   - name: http
#     port: 80
#     targetPort: 80 
#   type: NodePort 
#   selector:
#     app: du-pod1

---

</code></pre>
<h1 id="trouble-shooting"><a class="header" href="#trouble-shooting">trouble shooting</a></h1>
<pre><code class="language-bash"># the most important, kernel.sched_rt_runtime_us should be -1, it is setting for realtime, and for stalld
sysctl kernel.sched_rt_runtime_us

sysctl -w kernel.sched_rt_runtime_us=-1

ps -e -o uid,pid,ppid,cls,rtprio,pri,ni,cmd | grep 'stalld\|rcuc\|softirq\|worker\|bin_read\|dumgr\|duoam' 

oc adm must-gather \
--image=registry.redhat.io/openshift4/performance-addon-operator-must-gather-rhel8 

oc get performanceprofile wzh-performanceprofile -o yaml &gt; wzh-performanceprofile.output.yaml

oc describe node/worker-0 &gt; node.worker-0.output

oc describe mcp/worker-rt &gt; mcp.worker-rt.output

# if host device can't release ip address
cd /var/lib/cni/networks/host-device-du
rm -f 192.168.12.105

</code></pre>
<h1 id="profile-creator"><a class="header" href="#profile-creator">profile creator</a></h1>
<p>https://docs.openshift.com/container-platform/4.9/scalability_and_performance/cnf-create-performance-profiles.html</p>
<pre><code class="language-bash"># on helper
oc adm must-gather --image=quay.io/openshift-kni/performance-addon-operator-must-gather:4.9-snapshot --dest-dir=must-gather

podman run --rm --entrypoint performance-profile-creator quay.io/openshift-kni/performance-addon-operator:4.9-snapshot -h

podman run --rm --entrypoint performance-profile-creator -v /data/install/tmp/must-gather:/must-gather:z quay.io/openshift-kni/performance-addon-operator:4.9-snapshot --mcp-name=worker-rt --reserved-cpu-count=2 --topology-manager-policy=single-numa-node --rt-kernel=true --profile-name=wzh-performanceprofile --power-consumption-mode=ultra-low-latency --disable-ht=true  --must-gather-dir-path /must-gather &gt; my-performance-profile.yaml
# ---
# apiVersion: performance.openshift.io/v2
# kind: PerformanceProfile
# metadata:
#   name: wzh-performanceprofile
# spec:
#   additionalKernelArgs:
#   - audit=0
#   - idle=poll
#   - intel_idle.max_cstate=0
#   - mce=off
#   - nmi_watchdog=0
#   - nosmt
#   - processor.max_cstate=1
#   cpu:
#     isolated: 2-19
#     reserved: 0-1
#   machineConfigPoolSelector:
#     machineconfiguration.openshift.io/role: worker-rt
#   nodeSelector:
#     node-role.kubernetes.io/worker-rt: &quot;&quot;
#   numa:
#     topologyPolicy: single-numa-node
#   realTimeKernel:
#     enabled: true
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nep应用容器化"><a class="header" href="#nep应用容器化">NEP应用容器化</a></h1>
<p>NEP应用一般都是裸机应用，或者虚拟机应用。上容器平台，大家的做法一般都是直接做一个很大的容器，把容器当虚拟机用。虽然这种做法简单粗暴，完全不符合云原生的初衷，但是这个就是现实。所以容器化要一步一步的来。</p>
<p>这里就总结了一个NEP应用，逐步上云的过程，现在这个过程还在继续中，所以会不断的丰富和调整。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configmap方式注入license-service使用host-port-router方式暴露"><a class="header" href="#configmap方式注入license-service使用host-port-router方式暴露">configmap方式注入license, service使用host port, router方式暴露</a></h1>
<p>是用config map的方式，向容器注入license. 同时用host port + router的方式，暴露管理段服务。</p>
<pre><code class="language-bash"># license file 加载到config map中
oc create configmap license.for.nep  \
    --from-file=license=./3496531EC238AD91DED6DBA5BD6B.lic

# to updated config map
oc create configmap license.for.nep --from-file=license=./3496531EC238AD91DED6DBA5BD6B.lic -o yaml --dry-run=client | oc apply -f -

cat &lt;&lt; EOF &gt; /data/install/vbbu.yaml
---

apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: host-device-du
spec:
  config: '{
    &quot;cniVersion&quot;: &quot;0.3.0&quot;,
    &quot;type&quot;: &quot;host-device&quot;,
    &quot;device&quot;: &quot;xeth&quot;,
    &quot;ipam&quot;: {
      &quot;type&quot;: &quot;host-local&quot;,
      &quot;subnet&quot;: &quot;192.168.160.0/24&quot;,
      &quot;gateway&quot;: &quot;192.168.160.254&quot;,
      &quot;rangeStart&quot;: &quot;192.168.160.1&quot;,
      &quot;rangeEnd&quot;: &quot;192.168.160.1&quot;
    }
  }'


---

apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: host-device-du-ens
spec:
  config: '{
    &quot;cniVersion&quot;: &quot;0.3.0&quot;,
    &quot;type&quot;: &quot;host-device&quot;,
    &quot;device&quot;: &quot;enp103s0f0&quot;,
    &quot;ipam&quot;: {
      &quot;type&quot;: &quot;host-local&quot;,
      &quot;subnet&quot;: &quot;192.168.12.0/24&quot;,
      &quot;rangeStart&quot;: &quot;192.168.12.105&quot;,
      &quot;rangeEnd&quot;: &quot;192.168.12.106&quot;
    }
  }'



---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: du-deployment1
  labels:
    app: du-deployment1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: du-pod1
  template:
    metadata:
      labels:
        app: du-pod1
      annotations:
        k8s.v1.cni.cncf.io/networks: '[
          { &quot;name&quot;: &quot;host-device-du-ens&quot;,
            &quot;interface&quot;: &quot;veth11&quot; },
          { &quot;name&quot;: &quot;host-device-du&quot;,
            &quot;interface&quot;: &quot;xeth&quot; }
          ]'
      cpu-load-balancing.crio.io: &quot;true&quot;
    spec:
      runtimeClassName: performance-wzh-performanceprofile
      containers:
      - name: du-container1
        image: &quot;registry.ocp4.redhat.ren:5443/ocp4/du:v1-1623-wzh-01&quot;
        imagePullPolicy: IfNotPresent
        tty: true
        stdin: true
        env:
          - name: duNetProviderDriver
            value: &quot;host-netdevice&quot;
        # command: [&quot;/usr/sbin/init&quot;]
        # - sleep
        # - infinity
        securityContext:
            privileged: true
            capabilities:
                add:
                - CAP_SYS_ADMIN
        volumeMounts:
          - mountPath: /hugepages
            name: hugepage
          - name: lib-modules
            mountPath: /lib/modules
          - name: src
            mountPath: /usr/src
          - name: dev
            mountPath: /dev
          - name: cache-volume
            mountPath: /dev/shm
          - name: license-volume
            mountPath: /nep/lic
        resources:
          requests:
            cpu: 14
            memory: 64Gi
            hugepages-1Gi: 16Gi
          limits:
            cpu: 14
            memory: 64Gi
            hugepages-1Gi: 16Gi
      volumes:
        - name: hugepage
          emptyDir:
            medium: HugePages
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: src
          hostPath:
            path: /usr/src
        - name: dev
          hostPath:
            path: &quot;/dev&quot;
        - name: cache-volume
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
        - name: license-volume
          configMap:
            name: license.for.nep
            items:
            - key: license
              path: license.lic
      nodeSelector:
        node-role.kubernetes.io/master: &quot;&quot;

---

apiVersion: v1
kind: Service
metadata:
  name: du-http 
spec:
  ports:
  - name: http
    port: 80
    targetPort: 80 
    nodePort: 31071
  type: NodePort 
  selector:
    app: du-pod1

---

apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: du-http 
spec:
  port:
    targetPort: 80
  to:
    kind: Service
    name: du-http 

---

EOF

oc create -f /data/install/vbbu.yaml

# to restore
oc delete -f /data/install/vbbu.yaml

# open browser, to access vbbu console
# http://du-http-default.apps.ocp4s.redhat.ren/

# license file locates in /nep/lic/license.lic

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hostpath方式注入license-service使用host-port-router方式暴露"><a class="header" href="#hostpath方式注入license-service使用host-port-router方式暴露">hostpath方式注入license, service使用host port, router方式暴露</a></h1>
<p>用本机host path的方式，挂载到容器，从而注入license. 同时用host port + router的方式，暴露管理段服务。</p>
<pre><code class="language-bash">
# 创建host path
cat &lt;&lt; EOF &gt; /data/install/host-path.yaml
---
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 50-set-selinux-for-hostpath-nep-master
  labels:
    machineconfiguration.openshift.io/role: master
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
        - contents: |
            [Unit]
            Description=Set SELinux chcon for hostpath nep
            Before=kubelet.service

            [Service]
            Type=oneshot
            RemainAfterExit=yes
            ExecStartPre=-mkdir -p /var/nep/
            ExecStart=chcon -h unconfined_u:object_r:container_file_t /var/nep/

            [Install]
            WantedBy=multi-user.target
          enabled: true
          name: hostpath-nep.service
EOF
oc create -f /data/install/host-path.yaml

# restore
oc delete -f /data/install/host-path.yaml

cat &lt;&lt; EOF &gt; /data/install/vbbu.yaml
---
apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: host-device-du
spec:
  config: '{
    &quot;cniVersion&quot;: &quot;0.3.0&quot;,
    &quot;type&quot;: &quot;host-device&quot;,
    &quot;device&quot;: &quot;xeth&quot;,
    &quot;ipam&quot;: {
      &quot;type&quot;: &quot;host-local&quot;,
      &quot;subnet&quot;: &quot;192.168.160.0/24&quot;,
      &quot;gateway&quot;: &quot;192.168.160.254&quot;,
      &quot;rangeStart&quot;: &quot;192.168.160.1&quot;,
      &quot;rangeEnd&quot;: &quot;192.168.160.1&quot;
    }
  }'

---
apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: host-device-du-ens
spec:
  config: '{
    &quot;cniVersion&quot;: &quot;0.3.0&quot;,
    &quot;type&quot;: &quot;host-device&quot;,
    &quot;device&quot;: &quot;enp103s0f0&quot;,
    &quot;ipam&quot;: {
      &quot;type&quot;: &quot;host-local&quot;,
      &quot;subnet&quot;: &quot;192.168.12.0/24&quot;,
      &quot;rangeStart&quot;: &quot;192.168.12.105&quot;,
      &quot;rangeEnd&quot;: &quot;192.168.12.106&quot;
    }
  }'

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: du-deployment1
  labels:
    app: du-deployment1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: du-pod1
  template:
    metadata:
      labels:
        app: du-pod1
      annotations:
        k8s.v1.cni.cncf.io/networks: '[
          { &quot;name&quot;: &quot;host-device-du-ens&quot;,
            &quot;interface&quot;: &quot;veth11&quot; },
          { &quot;name&quot;: &quot;host-device-du&quot;,
            &quot;interface&quot;: &quot;xeth&quot; }
          ]'
      cpu-load-balancing.crio.io: &quot;true&quot;
    spec:
      runtimeClassName: performance-wzh-performanceprofile
      containers:
      - name: du-container1
        image: &quot;registry.ocp4.redhat.ren:5443/ocp4/du:v1-1623-wzh-01&quot;
        imagePullPolicy: IfNotPresent
        tty: true
        stdin: true
        env:
          - name: duNetProviderDriver
            value: &quot;host-netdevice&quot;
          - name: DEMO_ENV_NIC
            value: xeth
          - name: DEMO_ENV_IP
            value: &quot;192.168.100.22&quot;
          - name: DEMO_ENV_MASK
            value: 24
        command: [&quot;/usr/sbin/init&quot;]
        # - sleep
        # - infinity
        securityContext:
            privileged: true
            capabilities:
                add:
                - CAP_SYS_ADMIN
        volumeMounts:
          - mountPath: /hugepages
            name: hugepage
          - name: lib-modules
            mountPath: /lib/modules
          - name: src
            mountPath: /usr/src
          - name: dev
            mountPath: /dev
          - name: cache-volume
            mountPath: /dev/shm
          # - name: license-volume
          #   mountPath: /nep/lic
          - name: config
            mountPath: /nep
        resources:
          requests:
            cpu: 15
            memory: 64Gi
            hugepages-1Gi: 16Gi
          limits:
            cpu: 15
            memory: 64Gi
            hugepages-1Gi: 16Gi
      volumes:
        - name: hugepage
          emptyDir:
            medium: HugePages
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: src
          hostPath:
            path: /usr/src
        - name: config
          hostPath:
            path: /var/nep
        - name: dev
          hostPath:
            path: &quot;/dev&quot;
        - name: cache-volume
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
        # - name: license-volume
        #   configMap:
        #     name: license.for.nep
        #     items:
        #     - key: license
        #       path: license.lic
      nodeSelector:
        node-role.kubernetes.io/master: &quot;&quot;

---
apiVersion: v1
kind: Service
metadata:
  name: du-http 
spec:
  ports:
  - name: http
    port: 80
    targetPort: 80 
    nodePort: 31071
  type: NodePort 
  selector:
    app: du-pod1

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: du-http 
spec:
  port:
    targetPort: 80
  to:
    kind: Service
    name: du-http 

EOF

oc create -f /data/install/vbbu.yaml

# to restore
oc delete -f /data/install/vbbu.yaml

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="容器中使用systemd加载服务并注入env参数"><a class="header" href="#容器中使用systemd加载服务并注入env参数">容器中使用systemd加载服务并注入env参数</a></h1>
<p>客户希望在容器中，是用systemd做1号进程，从而让容器方案和裸机方案对齐。</p>
<pre><code class="language-bash">mkdir -p /data/systemd

cd /data/systemd
cat &lt;&lt; 'EOF' &gt; service.sh
#!/bin/bash

# Import our environment variables from systemd
for e in $(tr &quot;\000&quot; &quot;\n&quot; &lt; /proc/1/environ); do
  # if [[ $e == DEMO_ENV* ]]; then
    eval &quot;export $e&quot;
  # fi
done

echo $DEMO_ENV_NIC &gt; /demo.txt
echo $DEMO_ENV_IP &gt;&gt; /demo.txt
echo $DEMO_ENV_MASK &gt;&gt; /demo.txt

EOF
cat &lt;&lt; EOF &gt; vbbu.service
[Unit]
Description=vBBU Server
After=network.target

[Service]
Type=forking
User=root
WorkingDirectory=/root/
ExecStart=/service.sh

[Install]
WantedBy=multi-user.target
EOF

cat &lt;&lt; EOF &gt; ./vbbu.dockerfile
FROM docker.io/rockylinux/rockylinux:8

USER root
COPY service.sh /service.sh
RUN chmod +x /service.sh
COPY vbbu.service /etc/systemd/system/vbbu.service

RUN systemctl enable vbbu.service

entrypoint [&quot;/usr/sbin/init&quot;]
EOF

buildah bud -t quay.io/wangzheng422/qimgs:systemd -f vbbu.dockerfile .

podman run --rm \
  --env DEMO_ENV_NIC=eth1 \
  --env DEMO_ENV_IP=192.168.100.22 \
  --env DEMO_ENV_MASK=24 \
  quay.io/wangzheng422/qimgs:systemd

podman exec -it `podman ps | grep systemd | awk '{print $1}'` bash

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="设备驱动容器化方式加载"><a class="header" href="#设备驱动容器化方式加载">设备驱动容器化方式加载</a></h1>
<p>从容器向宿主机注入设备驱动，是用init container的方式，把驱动文件注入到容器中，然后容器向宿主机注入。</p>
<p>红帽官方的方案，是device kit/SRO，但是那个太复杂了，我们使用简单粗暴的方式实现。</p>
<ul>
<li>https://stackoverflow.com/questions/55291850/kubernetes-how-to-copy-a-cfg-file-into-container-before-contaner-running</li>
<li>https://access.redhat.com/solutions/4929021</li>
</ul>
<pre><code class="language-bash">
mkdir -p /data/wzh/fpga
cd /data/wzh/fpga

cat &lt;&lt; 'EOF' &gt; ./ocp4.install.sh
#!/bin/bash

if  chroot /host lsmod  | grep nr_drv &gt; /dev/null 2&gt;&amp;1
then
    echo NR Driver Module had loaded!
else
    echo Inserting NR Driver Module
    chroot /host rmmod nr_drv &gt; /dev/null 2&gt;&amp;1

    if [ $(uname -r) == &quot;4.18.0-305.19.1.rt7.91.el8_4.x86_64&quot; ];
    then
        echo insmod nr_drv_wr.ko ...
        /bin/cp -f nr_drv_wr.ko /host/tmp/nr_drv_wr.ko
        chroot /host insmod /tmp/nr_drv_wr.ko load_xeth=1
        /bin/rm -f /host/tmp/nr_drv_wr.ko

        CON_NAME=`chroot /host nmcli -g GENERAL.CONNECTION dev show xeth`

        chroot /host nmcli connection modify &quot;$CON_NAME&quot; con-name xeth
        chroot /host nmcli connection modify xeth ipv4.method disabled ipv6.method disabled
        chroot /host nmcli dev conn xeth
    else
        echo insmod nr_drv_ko Failed!
    fi

fi
EOF

cat &lt;&lt; EOF &gt; ./fpga.dockerfile
FROM docker.io/busybox:1.34

USER root
COPY Driver.PKG /Driver.PKG

COPY ocp4.install.sh /ocp4.install.sh
RUN chmod +x /ocp4.install.sh

WORKDIR /
EOF

buildah bud -t registry.ocp4.redhat.ren:5443/nep/fgpa-driver:v04 -f fpga.dockerfile .

buildah push registry.ocp4.redhat.ren:5443/nep/fgpa-driver:v04

cat &lt;&lt; EOF &gt; /data/install/fpga.driver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fpga-driver
  # namespace: default
  labels:
    app: fpga-driver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fpga-driver
  template:
    metadata:
      labels:
        app: fpga-driver
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: &quot;app&quot;
                    operator: In
                    values:
                    - fpga-driver
              topologyKey: &quot;kubernetes.io/hostname&quot;
      nodeselector:
        node-role.kubernetes.io/master: &quot;&quot;
      # restartPolicy: Never
      initContainers:
      - name: copy
        image: registry.ocp4.redhat.ren:5443/nep/fgpa-driver:v04
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;tar zvxf /Driver.PKG --strip 1 -C /nep/driver/ &amp;&amp; /bin/cp -f /ocp4.install.sh /nep/driver/ &quot;]
        imagePullPolicy: Always
        volumeMounts:
        - name: driver-files
          mountPath: /nep/driver/
      containers:
      - name: driver
        image: registry.redhat.io/rhel8/support-tools:8.4
        # imagePullPolicy: Always
        command: [ &quot;/usr/bin/bash&quot;,&quot;-c&quot;,&quot;cd /nep/driver/ &amp;&amp; bash ./ocp4.install.sh &amp;&amp; sleep infinity &quot; ]
        # command: [ &quot;/usr/bin/bash&quot;,&quot;-c&quot;,&quot;tail -f /dev/null || true &quot; ]
        resources:
          requests:
            cpu: 10m
            memory: 20Mi
        securityContext:
          privileged: true
          runAsUser: 0
        volumeMounts:
        - name: driver-files
          mountPath: /nep/driver/
        - name: host
          mountPath: /host
      volumes: 
      - name: driver-files
        emptyDir: {}
      - name: host
        hostPath:
          path: /
          type: Directory
EOF
oc create -f /data/install/fpga.driver.yaml

# to restore
oc delete -f /data/install/fpga.driver.yaml


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="基础镜像制作"><a class="header" href="#基础镜像制作">基础镜像制作</a></h1>
<p>帮助客户梳理基础镜像构建，开始构建CI/CD流程.</p>
<p>基于UBI的镜像，有订阅证书问题，现在准备一下。</p>
<pre><code class="language-bash"># on a vultr host, rockylinux
mkdir -p /data/rhel8/entitle
cd /data/rhel8/entitle

# goto https://access.redhat.com/management/subscriptions
# search employee sku, find a system, go into, and download from subscription
# or goto: https://access.redhat.com/management/systems/4d1e4cc0-2c99-4431-99ce-2f589a24ea11/subscriptions
dnf install -y unzip 
unzip *
unzip consumer_export.zip
find . -name *.pem -exec cp {} ./ \;

mkdir -p /data/dockerfile/
cd /data/dockerfile/

ls /data/rhel8/entitle/*.pem | sed -n '2p' | xargs -I DEMO /bin/cp -f DEMO ./ 

</code></pre>
<h2 id="redhat-ubi8"><a class="header" href="#redhat-ubi8">redhat ubi8</a></h2>
<p>UBI的容器，有注入和使用证书的技巧。</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /data/dockerfile/nep.redhat.ubi8.dockerfile
FROM registry.access.redhat.com/ubi8

COPY *.pem /etc/pki/entitlement/entitlement.pem
COPY *.pem /etc/pki/entitlement/entitlement-key.pem

RUN dnf -y update || true &amp;&amp; \
  sed -i 's|enabled=1|enabled=0|g' /etc/yum/pluginconf.d/subscription-manager.conf &amp;&amp; \
  sed -i 's|%(ca_cert_dir)sredhat-uep.pem|/etc/rhsm/ca/redhat-uep.pem|g' /etc/yum.repos.d/redhat.repo &amp;&amp; \
  dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm &amp;&amp; \
  dnf -y update &amp;&amp; \
  dnf -y install net-tools pciutils lksctp-tools iptraf-ng htop vim tcpdump wget bzip2 lrzsz dhcp-server dhcp-client  &amp;&amp; \
  dnf -y clean all 

EOF
buildah bud -t quay.io/nep/base-image:ubi8 -f /data/dockerfile/nep.redhat.ubi8.dockerfile .

</code></pre>
<h2 id="redhat-ubi7"><a class="header" href="#redhat-ubi7">redhat ubi7</a></h2>
<p>UBI的容器，有注入和使用证书的技巧。</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /data/dockerfile/nep.redhat.ubi.7.dockerfile
FROM registry.access.redhat.com/ubi7/ubi

COPY *.pem /etc/pki/entitlement/entitlement.pem
COPY *.pem /etc/pki/entitlement/entitlement-key.pem

RUN sed -i 's|%(ca_cert_dir)sredhat-uep.pem|/etc/rhsm/ca/redhat-uep.pem|g' /etc/rhsm/rhsm.conf &amp;&amp; \
  yum -y update || true &amp;&amp; \
  sed -i 's|enabled=1|enabled=0|g' /etc/yum/pluginconf.d/subscription-manager.conf &amp;&amp; \
  sed -i 's|%(ca_cert_dir)sredhat-uep.pem|/etc/rhsm/ca/redhat-uep.pem|g' /etc/yum.repos.d/redhat.repo &amp;&amp; \
  yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm &amp;&amp; \
  yum -y update &amp;&amp; \
  yum -y install net-tools pciutils lksctp-tools iptraf-ng htop vim tcpdump wget bzip2 lrzsz dhcp &amp;&amp; \
  yum clean all 

EOF
buildah bud -t quay.io/nep/base-image:ubi7 -f /data/dockerfile/nep.redhat.ubi.7.dockerfile .

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="helm-chart--helm-operator-制作"><a class="header" href="#helm-chart--helm-operator-制作">helm chart / helm operator 制作</a></h1>
<h2 id="build-helm-operator"><a class="header" href="#build-helm-operator">build helm operator</a></h2>
<pre><code class="language-bash">mkdir -p /data/down
cd /data/down
wget https://mirror.openshift.com/pub/openshift-v4/clients/operator-sdk/latest/operator-sdk-linux-x86_64.tar.gz
tar zvxf operator-sdk-linux-x86_64.tar.gz
install operator-sdk /usr/local/bin/

operator-sdk init --plugins helm --help

mkdir -p /data/helm
cd /data/helm

# 初始化项目
operator-sdk init \
    --plugins=helm \
    --project-name nep-helm-operator \
    --domain=nep.com \
    --group=apps \
    --version=v1alpha1 \
    --kind=VBBU 

make bundle
# operator-sdk generate kustomize manifests -q

# Display name for the operator (required):
# &gt; nep vBBU

# Description for the operator (required):
# &gt; nep vRAN application including fpga driver, vCU, vDU

# Provider's name for the operator (required):
# &gt; nep

# Any relevant URL for the provider name (optional):
# &gt; na.nep.com

# Comma-separated list of keywords for your operator (required):
# &gt; nep,vbbu,vran,vcu,vdu

# Comma-separated list of maintainers and their emails (e.g. 'name1:email1, name2:email2') (required):
# &gt;
# No list provided.
# Comma-separated list of maintainers and their emails (e.g. 'name1:email1, name2:email2') (required):
# &gt; wangzheng:wangzheng422@foxmail.com
# cd config/manager &amp;&amp; /data/helm/bin/kustomize edit set image controller=quay.io/nep/nep-helm-operator:latest
# /data/helm/bin/kustomize build config/manifests | operator-sdk generate bundle -q --overwrite --version 0.0.1
# INFO[0001] Creating bundle.Dockerfile
# INFO[0001] Creating bundle/metadata/annotations.yaml
# INFO[0001] Bundle metadata generated suceessfully
# operator-sdk bundle validate ./bundle
# INFO[0000] All validation tests have completed successfully

cd /data/helm/helm-charts/vbbu
helm lint

dnf install -y podman-docker

cd /data/helm/
make docker-build
# docker build -t quay.io/nep/nep-helm-operator:v01 .
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# STEP 1/5: FROM registry.redhat.io/openshift4/ose-helm-operator:v4.9
# STEP 2/5: ENV HOME=/opt/helm
# --&gt; 1eec2f9c094
# STEP 3/5: COPY watches.yaml ${HOME}/watches.yaml
# --&gt; 1836589a08c
# STEP 4/5: COPY helm-charts  ${HOME}/helm-charts
# --&gt; b6cd9f24e47
# STEP 5/5: WORKDIR ${HOME}
# COMMIT quay.io/nep/nep-helm-operator:v01
# --&gt; 1f9bcc4cecc
# Successfully tagged quay.io/nep/nep-helm-operator:v01
# 1f9bcc4cecc55e68170e2a6f45dad7b318018df8bf3989bd990f567e3ccdfcd9

make docker-push
# docker push quay.io/nep/nep-helm-operator:v01
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# Getting image source signatures
# Copying blob 8cd9b2cfbe06 skipped: already exists
# Copying blob 5bc03dec6239 skipped: already exists
# Copying blob 525ed45dbdb1 skipped: already exists
# Copying blob 758ace4ace74 skipped: already exists
# Copying blob deb6b0f93acd skipped: already exists
# Copying blob ac83cd3b61fd skipped: already exists
# Copying blob 12f964d7475b [--------------------------------------] 0.0b / 0.0b
# Copying config 1f9bcc4cec [--------------------------------------] 0.0b / 4.0KiB
# Writing manifest to image destination
# Copying config 1f9bcc4cec [--------------------------------------] 0.0b / 4.0KiB
# Writing manifest to image destination
# Storing signatures

make bundle-build BUNDLE_IMG=quay.io/nep/nep-helm-operator:bundle-v01
# docker build -f bundle.Dockerfile -t quay.io/nep/nep-helm-operator:bundle-v01 .
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# STEP 1/14: FROM scratch
# STEP 2/14: LABEL operators.operatorframework.io.bundle.mediatype.v1=registry+v1
# --&gt; Using cache b67edfbd23d6ba9c3f484a1e01f9da79fbffdc44e913423e2f616e477df372e1
# --&gt; b67edfbd23d
# STEP 3/14: LABEL operators.operatorframework.io.bundle.manifests.v1=manifests/
# --&gt; Using cache f2eef5180d3c9c63f40a98880ec95088b8395845e0f90960a194326d77a6f3b4
# --&gt; f2eef5180d3
# STEP 4/14: LABEL operators.operatorframework.io.bundle.metadata.v1=metadata/
# --&gt; Using cache 6fc10718a71e30d31cc652b47ac27ca87901ff4fda17a25e2d6bc53344e50673
# --&gt; 6fc10718a71
# STEP 5/14: LABEL operators.operatorframework.io.bundle.package.v1=nep-helm-operator
# --&gt; Using cache 6664d1d6c64c0954c18a432194845551e5a0c6f9bba33175d77c8791e2b0f6e0
# --&gt; 6664d1d6c64
# STEP 6/14: LABEL operators.operatorframework.io.bundle.channels.v1=alpha
# --&gt; Using cache 32878b9e903851bb51b6c0635c77112b4244f4ce7e9d8a7b0a0d8cf7fe7bbe0e
# --&gt; 32878b9e903
# STEP 7/14: LABEL operators.operatorframework.io.metrics.builder=operator-sdk-v1.10.1-ocp
# --&gt; Using cache c5482c80a3287494a5f35ee8df782f4499ad6def2aaa55652e5fc57d4dfa8f0d
# --&gt; c5482c80a32
# STEP 8/14: LABEL operators.operatorframework.io.metrics.mediatype.v1=metrics+v1
# --&gt; Using cache 68822f2fae03c5efc8b980882f66e870d8942d80dbf697e3d784c46f95c50437
# --&gt; 68822f2fae0
# STEP 9/14: LABEL operators.operatorframework.io.metrics.project_layout=helm.sdk.operatorframework.io/v1
# --&gt; Using cache a85519d2774008b3071baf6098ec59561102ef1f337acd19b2c7ef739ebae89e
# --&gt; a85519d2774
# STEP 10/14: LABEL operators.operatorframework.io.test.mediatype.v1=scorecard+v1
# --&gt; Using cache 17a1b08e1dca2295f98e3288d592a08636d15d7461e25e11744a499160a1546c
# --&gt; 17a1b08e1dc
# STEP 11/14: LABEL operators.operatorframework.io.test.config.v1=tests/scorecard/
# --&gt; Using cache 9b6a20b0ff75b501a321fe4fbdfd1d284763e65596dc85675f119e5e3de69657
# --&gt; 9b6a20b0ff7
# STEP 12/14: COPY bundle/manifests /manifests/
# --&gt; Using cache ff3aa5b299dae11f464d8ad56f4ae5130974e1cebd0cf273bc03aba11fcb7377
# --&gt; ff3aa5b299d
# STEP 13/14: COPY bundle/metadata /metadata/
# --&gt; Using cache 19395ef3259bbb4e1f5da9616195139698a3ef18e7f904a2a1cd7515cd9829f3
# --&gt; 19395ef3259
# STEP 14/14: COPY bundle/tests/scorecard /tests/scorecard/
# --&gt; Using cache 2268eb0a731f424f70e5b46222a1accd5344560ac9ab609ca3ccb5a4d0cd6669
# COMMIT quay.io/nep/nep-helm-operator:bundle-v01
# --&gt; 2268eb0a731
# Successfully tagged quay.io/nep/nep-helm-operator:bundle-v01
# Successfully tagged quay.io/nep/nep-helm-operator-bundle:v0.0.1
# 2268eb0a731f424f70e5b46222a1accd5344560ac9ab609ca3ccb5a4d0cd6669


make bundle-push BUNDLE_IMG=quay.io/nep/nep-helm-operator:bundle-v01
# make docker-push IMG=quay.io/nep/nep-helm-operator:bundle-v01
# make[1]: Entering directory '/data/helm'
# docker push quay.io/nep/nep-helm-operator:bundle-v01
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# Getting image source signatures
# Copying blob 24b54377030e skipped: already exists
# Copying blob 1929cd83db02 skipped: already exists
# Copying blob 44ef63131a17 [--------------------------------------] 0.0b / 0.0b
# Copying config 2268eb0a73 done
# Writing manifest to image destination
# Copying config 2268eb0a73 [--------------------------------------] 0.0b / 3.3KiB
# Writing manifest to image destination
# Storing signatures
# make[1]: Leaving directory '/data/helm'

make catalog-build CATALOG_IMG=quay.io/nep/nep-helm-operator:catalog-v01  BUNDLE_IMG=quay.io/nep/nep-helm-operator:bundle-v01 
# ./bin/opm index add --mode semver --tag quay.io/nep/nep-helm-operator:catalog-v01 --bundles quay.io/nep/nep-helm-operator:bundle-v01
# INFO[0000] building the index                            bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;
# INFO[0000] resolved name: quay.io/nep/nep-helm-operator:bundle-v01
# INFO[0000] fetched                                       digest=&quot;sha256:1365e5913f05b733124a2a88c3113899db0c42f62b5758477577ef2117aff09f&quot;
# INFO[0000] fetched                                       digest=&quot;sha256:be008c9c2b4f2c031b301174608accb8622c8d843aba2d1af4d053d8b00373c2&quot;
# INFO[0000] fetched                                       digest=&quot;sha256:2268eb0a731f424f70e5b46222a1accd5344560ac9ab609ca3ccb5a4d0cd6669&quot;
# INFO[0000] fetched                                       digest=&quot;sha256:d8e28b323fec2e4de5aecfb46c4ce3e315e20f49b78f43eb7a1d657798695655&quot;
# INFO[0000] fetched                                       digest=&quot;sha256:c19ac761be31fa163ea3da95cb63fc0c2aaca3b316bfb049f6ee36f77522d323&quot;
# INFO[0001] unpacking layer: {application/vnd.docker.image.rootfs.diff.tar.gzip sha256:d8e28b323fec2e4de5aecfb46c4ce3e315e20f49b78f43eb7a1d657798695655 2985 [] map[] &lt;nil&gt;}
# INFO[0001] unpacking layer: {application/vnd.docker.image.rootfs.diff.tar.gzip sha256:c19ac761be31fa163ea3da95cb63fc0c2aaca3b316bfb049f6ee36f77522d323 398 [] map[] &lt;nil&gt;}
# INFO[0001] unpacking layer: {application/vnd.docker.image.rootfs.diff.tar.gzip sha256:be008c9c2b4f2c031b301174608accb8622c8d843aba2d1af4d053d8b00373c2 438 [] map[] &lt;nil&gt;}
# INFO[0001] Could not find optional dependencies file     dir=bundle_tmp582129875 file=bundle_tmp582129875/metadata load=annotations
# INFO[0001] found csv, loading bundle                     dir=bundle_tmp582129875 file=bundle_tmp582129875/manifests load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=apps.nep.com_vbbus.yaml load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=nep-helm-operator-controller-manager-metrics-service_v1_service.yaml load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=nep-helm-operator-manager-config_v1_configmap.yaml load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=nep-helm-operator-metrics-reader_rbac.authorization.k8s.io_v1_clusterrole.yaml load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=nep-helm-operator.clusterserviceversion.yaml load=bundle
# INFO[0001] Generating dockerfile                         bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;
# INFO[0001] writing dockerfile: index.Dockerfile322782265  bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;
# INFO[0001] running podman build                          bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;
# INFO[0001] [podman build --format docker -f index.Dockerfile322782265 -t quay.io/nep/nep-helm-operator:catalog-v01 .]  bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;

make catalog-push CATALOG_IMG=quay.io/nep/nep-helm-operator:catalog-v01
# make docker-push IMG=quay.io/nep/nep-helm-operator:catalog-v01
# make[1]: Entering directory '/data/helm'
# docker push quay.io/nep/nep-helm-operator:catalog-v01
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# Getting image source signatures
# Copying blob 8a20ae5d4166 done
# Copying blob a98a386b6ec2 skipped: already exists
# Copying blob 4e7f383eb531 skipped: already exists
# Copying blob bc276c40b172 skipped: already exists
# Copying blob b15904f6a114 skipped: already exists
# Copying blob 86aadf4df7dc skipped: already exists
# Copying config 5d5d1c219c done
# Writing manifest to image destination
# Storing signatures
# make[1]: Leaving directory '/data/helm'

export OPERATOR_VERION=v04

make docker-build IMG=quay.io/nep/nep-helm-operator:$OPERATOR_VERION

make docker-push IMG=quay.io/nep/nep-helm-operator:$OPERATOR_VERION

make bundle IMG=quay.io/nep/nep-helm-operator:$OPERATOR_VERION

make bundle-build bundle-push catalog-build catalog-push \
    BUNDLE_IMG=quay.io/nep/nep-helm-operator:bundle-$OPERATOR_VERION \
    CATALOG_IMG=quay.io/nep/nep-helm-operator:catalog-$OPERATOR_VERION

# on openshift helper node
cat &lt;&lt; EOF &gt; /data/install/nep.catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: nep
  namespace: openshift-marketplace
spec:
  displayName: nep
  publisher: nep
  sourceType: grpc
  image: ghcr.io/wangzheng422/nep-helm-operator:catalog-2021-12-03-0504
  updateStrategy:
    registryPoll:
      interval: 10m
EOF
oc create -f /data/install/nep.catalog.yaml
# to restore
oc delete -f /data/install/nep.catalog.yaml

</code></pre>
<h2 id="helm-repository"><a class="header" href="#helm-repository">helm repository</a></h2>
<p>https://medium.com/@mattiaperi/create-a-public-helm-chart-repository-with-github-pages-49b180dbb417</p>
<pre><code class="language-bash"># try to build the repo, and add it into github action
# mkdir -p /data/helm/helm-repo
cd /data/helm/helm-repo

helm package ../helm-charts/*

helm repo index --url https://wangzheng422.github.io/nep-helm-operator/ .

# try to use the repo
helm repo add myhelmrepo https://wangzheng422.github.io/nep-helm-operator/

helm repo list
# NAME            URL
# myhelmrepo      https://wangzheng422.github.io/nep-helm-operator/

helm search repo vbbu
# NAME            CHART VERSION   APP VERSION     DESCRIPTION
# myhelmrepo/vbbu 0.1.0           1.16.0          A Helm chart for Kubernetes

# for ocp, if you are disconnected
cat &lt;&lt; EOF &gt; /data/install/helm.nep.yaml
apiVersion: helm.openshift.io/v1beta1
kind: HelmChartRepository
metadata:
  name: nep-helm-charts-wzh
spec:
 # optional name that might be used by console
  name: nep-helm-charts-wzh
  connectionConfig:
    url: http://nexus.ocp4.redhat.ren:8082/repository/wangzheng422.github.io/
EOF
oc create -f /data/install/helm.nep.yaml

</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
